"""
æ´å¯Ÿç”Ÿæˆæ¨¡å—
åŸºäºèšç±»ç»“æœç”Ÿæˆä¸šåŠ¡æ´å¯Ÿå’Œè¡ŒåŠ¨å»ºè®®
"""

import logging
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class ClusterInsight:
    """å•ä¸ªç°‡çš„æ´å¯Ÿ"""
    cluster_id: int
    cluster_name: str
    key_insights: List[str]
    pain_points: List[str]
    opportunities: List[str]
    recommended_actions: List[str]
    priority_score: float
    confidence_level: str

@dataclass
class BusinessInsight:
    """ä¸šåŠ¡æ´å¯ŸæŠ¥å‘Š"""
    analysis_timestamp: datetime
    total_clusters: int
    total_samples: int
    overall_sentiment: str
    dominant_themes: List[str]
    top_pain_points: List[str]
    key_opportunities: List[str]
    strategic_recommendations: List[str]
    cluster_insights: List[ClusterInsight]
    action_priority_matrix: List[Dict[str, Any]]
    model_name: str = "insights_generator"

class InsightsGenerator:
    """æ´å¯Ÿç”Ÿæˆå™¨"""
    
    def __init__(self, llm_analyzer, provider: str = "openai"):
        self.llm_analyzer = llm_analyzer
        self.provider = provider
    
    def generate_insights(self, clustering_analysis: Any, extractions: List[Any] = None) -> Optional[BusinessInsight]:
        """ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ"""
        try:
            logger.info("å¼€å§‹ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ...")
            
            # ç”Ÿæˆæ•´ä½“æ´å¯Ÿ
            overall_insights = self._generate_overall_insights(clustering_analysis, extractions)
            
            # ç”Ÿæˆç°‡çº§æ´å¯Ÿ
            cluster_insights = self._generate_cluster_insights(clustering_analysis, extractions)
            
            # ç”Ÿæˆè¡ŒåŠ¨ä¼˜å…ˆçº§çŸ©é˜µ
            action_matrix = self._generate_action_priority_matrix(cluster_insights)
            
            # åˆ›å»ºä¸šåŠ¡æ´å¯ŸæŠ¥å‘Š
            business_insight = BusinessInsight(
                analysis_timestamp=datetime.now(),
                total_clusters=clustering_analysis.n_clusters,
                total_samples=sum(cluster.member_count for cluster in clustering_analysis.cluster_results),
                overall_sentiment=overall_insights.get('overall_sentiment', 'neutral'),
                dominant_themes=overall_insights.get('dominant_themes', []),
                top_pain_points=overall_insights.get('top_pain_points', []),
                key_opportunities=overall_insights.get('key_opportunities', []),
                strategic_recommendations=overall_insights.get('strategic_recommendations', []),
                cluster_insights=cluster_insights,
                action_priority_matrix=action_matrix
            )
            
            logger.info("ä¸šåŠ¡æ´å¯Ÿç”Ÿæˆå®Œæˆ")
            return business_insight
            
        except Exception as e:
            logger.error(f"æ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def _generate_overall_insights(self, clustering_analysis: Any, extractions: List[Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´ä½“æ´å¯Ÿ"""
        try:
            # å‡†å¤‡æ•´ä½“åˆ†ææ•°æ®
            analysis_data = self._prepare_overall_analysis_data(clustering_analysis, extractions)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_overall_insights_prompt(analysis_data)
            
            # è°ƒç”¨LLMç”Ÿæˆæ´å¯Ÿ
            result = self.llm_analyzer._call_llm(
                prompt=prompt,
                provider=self.provider,
                analysis_type="business_insights"
            )
            
            if "error" in result:
                logger.error(f"æ•´ä½“æ´å¯Ÿç”Ÿæˆå¤±è´¥: {result['error']}")
                return self._get_default_overall_insights()
            
            # è§£æç»“æœ
            insights = self._parse_insights_result(result.get('content', ''))
            return insights
            
        except Exception as e:
            logger.error(f"æ•´ä½“æ´å¯Ÿç”Ÿæˆå¼‚å¸¸: {str(e)}")
            return self._get_default_overall_insights()
    
    def _generate_cluster_insights(self, clustering_analysis: Any, extractions: List[Any] = None) -> List[ClusterInsight]:
        """ç”Ÿæˆç°‡çº§æ´å¯Ÿ"""
        cluster_insights = []
        
        for cluster in clustering_analysis.cluster_results:
            try:
                # å‡†å¤‡ç°‡åˆ†ææ•°æ®
                cluster_data = self._prepare_cluster_analysis_data(cluster, extractions)
                
                # æ„å»ºæç¤ºè¯
                prompt = self._build_cluster_insights_prompt(cluster_data)
                
                # è°ƒç”¨LLMç”Ÿæˆæ´å¯Ÿ
                result = self.llm_analyzer._call_llm(
                    prompt=prompt,
                    provider=self.provider,
                    analysis_type="cluster_insights"
                )
                
                if "error" in result:
                    logger.warning(f"ç°‡ {cluster.cluster_id} æ´å¯Ÿç”Ÿæˆå¤±è´¥: {result['error']}")
                    cluster_insight = self._get_default_cluster_insight(cluster)
                else:
                    # è§£æç»“æœ
                    insights_data = self._parse_cluster_insights_result(result.get('content', ''))
                    cluster_insight = self._create_cluster_insight(cluster, insights_data)
                
                cluster_insights.append(cluster_insight)
                
            except Exception as e:
                logger.error(f"ç°‡ {cluster.cluster_id} æ´å¯Ÿç”Ÿæˆå¼‚å¸¸: {str(e)}")
                cluster_insights.append(self._get_default_cluster_insight(cluster))
        
        return cluster_insights
    
    def _prepare_overall_analysis_data(self, clustering_analysis: Any, extractions: List[Any] = None) -> Dict[str, Any]:
        """å‡†å¤‡æ•´ä½“åˆ†ææ•°æ®"""
        data = {
            "total_clusters": clustering_analysis.n_clusters,
            "total_samples": sum(cluster.member_count for cluster in clustering_analysis.cluster_results),
            "silhouette_score": clustering_analysis.silhouette_score,
            "clusters": []
        }
        
        for cluster in clustering_analysis.cluster_results:
            cluster_info = {
                "cluster_id": cluster.cluster_id,
                "member_count": cluster.member_count,
                "keywords": cluster.keywords,
                "dominant_sentiment": cluster.dominant_sentiment,
                "avg_sentiment_score": cluster.avg_sentiment_score,
                "representative_samples": cluster.representative_samples
            }
            data["clusters"].append(cluster_info)
        
        return data
    
    def _prepare_cluster_analysis_data(self, cluster: Any, extractions: List[Any] = None) -> Dict[str, Any]:
        """å‡†å¤‡ç°‡åˆ†ææ•°æ®"""
        return {
            "cluster_id": cluster.cluster_id,
            "member_count": cluster.member_count,
            "keywords": cluster.keywords,
            "dominant_sentiment": cluster.dominant_sentiment,
            "avg_sentiment_score": cluster.avg_sentiment_score,
            "representative_samples": cluster.representative_samples,
            "avg_similarity": cluster.avg_similarity
        }
    
    def _build_overall_insights_prompt(self, analysis_data: Dict[str, Any]) -> str:
        """æ„å»ºæ•´ä½“æ´å¯Ÿæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆï¼Œéœ€è¦åŸºäºRedditç¤¾åŒºè®¨è®ºçš„èšç±»åˆ†æç»“æœç”Ÿæˆä¸šåŠ¡æ´å¯Ÿã€‚

## åˆ†ææ•°æ®
- æ€»ç°‡æ•°: {analysis_data['total_clusters']}
- æ€»æ ·æœ¬æ•°: {analysis_data['total_samples']}
- èšç±»è´¨é‡(è½®å»“ç³»æ•°): {analysis_data['silhouette_score']:.3f}

## å„ç°‡ç‰¹å¾
{self._format_clusters_for_prompt(analysis_data['clusters'])}

è¯·åŸºäºä»¥ä¸Šæ•°æ®ç”Ÿæˆä»¥ä¸‹æ´å¯Ÿï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼š

1. overall_sentiment: æ•´ä½“æƒ…æ„Ÿå€¾å‘ (positive/negative/neutral/mixed)
2. dominant_themes: ä¸»å¯¼ä¸»é¢˜åˆ—è¡¨ (æœ€å¤š5ä¸ª)
3. top_pain_points: ä¸»è¦ç—›ç‚¹åˆ—è¡¨ (æœ€å¤š5ä¸ª)
4. key_opportunities: å…³é”®æœºä¼šåˆ—è¡¨ (æœ€å¤š5ä¸ª)
5. strategic_recommendations: æˆ˜ç•¥å»ºè®®åˆ—è¡¨ (æœ€å¤š5ä¸ª)

JSONæ ¼å¼ï¼š
{{
    "overall_sentiment": "æ•´ä½“æƒ…æ„Ÿ",
    "dominant_themes": ["ä¸»é¢˜1", "ä¸»é¢˜2"],
    "top_pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2"],
    "key_opportunities": ["æœºä¼š1", "æœºä¼š2"],
    "strategic_recommendations": ["å»ºè®®1", "å»ºè®®2"]
}}"""
    
    def _build_cluster_insights_prompt(self, cluster_data: Dict[str, Any]) -> str:
        """æ„å»ºç°‡æ´å¯Ÿæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆï¼Œéœ€è¦ä¸ºä»¥ä¸‹Redditè®¨è®ºç°‡ç”Ÿæˆæ·±åº¦æ´å¯Ÿã€‚

## ç°‡ä¿¡æ¯
- ç°‡ID: {cluster_data['cluster_id']}
- æ ·æœ¬æ•°é‡: {cluster_data['member_count']}
- å…³é”®è¯: {', '.join(cluster_data['keywords'])}
- ä¸»å¯¼æƒ…æ„Ÿ: {cluster_data['dominant_sentiment']}
- æƒ…æ„Ÿå¼ºåº¦: {cluster_data['avg_sentiment_score']:.2f}

## ä»£è¡¨æ ·æœ¬
{self._format_samples_for_prompt(cluster_data['representative_samples'])}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä»¥ä¸‹æ´å¯Ÿï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼š

1. cluster_name: ç°‡çš„ç®€æ´åç§°
2. key_insights: å…³é”®æ´å¯Ÿåˆ—è¡¨ (æœ€å¤š3ä¸ª)
3. pain_points: ç—›ç‚¹åˆ—è¡¨ (æœ€å¤š3ä¸ª)
4. opportunities: æœºä¼šåˆ—è¡¨ (æœ€å¤š3ä¸ª)
5. recommended_actions: å»ºè®®è¡ŒåŠ¨åˆ—è¡¨ (æœ€å¤š3ä¸ª)
6. priority_score: ä¼˜å…ˆçº§åˆ†æ•° (0-10)
7. confidence_level: ç½®ä¿¡åº¦ (high/medium/low)

JSONæ ¼å¼ï¼š
{{
    "cluster_name": "ç°‡åç§°",
    "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2"],
    "pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2"],
    "opportunities": ["æœºä¼š1", "æœºä¼š2"],
    "recommended_actions": ["è¡ŒåŠ¨1", "è¡ŒåŠ¨2"],
    "priority_score": 8.5,
    "confidence_level": "high"
}}"""
    
    def _format_clusters_for_prompt(self, clusters: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ç°‡ä¿¡æ¯ç”¨äºæç¤ºè¯"""
        formatted = []
        for cluster in clusters:
            formatted.append(f"""
ç°‡ {cluster['cluster_id']}:
- æ ·æœ¬æ•°: {cluster['member_count']}
- å…³é”®è¯: {', '.join(cluster['keywords'])}
- æƒ…æ„Ÿ: {cluster['dominant_sentiment']} ({cluster['avg_sentiment_score']:.2f})
""")
        return '\n'.join(formatted)
    
    def _format_samples_for_prompt(self, samples: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ ·æœ¬ä¿¡æ¯ç”¨äºæç¤ºè¯"""
        formatted = []
        for i, sample in enumerate(samples[:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ ·æœ¬
            text_preview = sample.get('text_preview', '')[:100]
            formatted.append(f"æ ·æœ¬ {i+1}: {text_preview}...")
        return '\n'.join(formatted)
    
    def _parse_insights_result(self, content: str) -> Dict[str, Any]:
        """è§£ææ•´ä½“æ´å¯Ÿç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            if content.strip().startswith('{'):
                return json.loads(content.strip())
            
            # å°è¯•ä»ä»£ç å—ä¸­æå–JSON
            import re
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            
            # å°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            
            logger.warning(f"æ— æ³•è§£ææ•´ä½“æ´å¯ŸJSON: {content[:200]}...")
            return self._get_default_overall_insights()
            
        except json.JSONDecodeError as e:
            logger.error(f"æ•´ä½“æ´å¯ŸJSONè§£æå¤±è´¥: {str(e)}")
            return self._get_default_overall_insights()
    
    def _parse_cluster_insights_result(self, content: str) -> Dict[str, Any]:
        """è§£æç°‡æ´å¯Ÿç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            if content.strip().startswith('{'):
                return json.loads(content.strip())
            
            # å°è¯•ä»ä»£ç å—ä¸­æå–JSON
            import re
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            
            # å°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            
            logger.warning(f"æ— æ³•è§£æç°‡æ´å¯ŸJSON: {content[:200]}...")
            return self._get_default_cluster_insights_data()
            
        except json.JSONDecodeError as e:
            logger.error(f"ç°‡æ´å¯ŸJSONè§£æå¤±è´¥: {str(e)}")
            return self._get_default_cluster_insights_data()
    
    def _create_cluster_insight(self, cluster: Any, insights_data: Dict[str, Any]) -> ClusterInsight:
        """åˆ›å»ºç°‡æ´å¯Ÿå¯¹è±¡"""
        return ClusterInsight(
            cluster_id=cluster.cluster_id,
            cluster_name=insights_data.get('cluster_name', f'ç°‡ {cluster.cluster_id}'),
            key_insights=insights_data.get('key_insights', []),
            pain_points=insights_data.get('pain_points', []),
            opportunities=insights_data.get('opportunities', []),
            recommended_actions=insights_data.get('recommended_actions', []),
            priority_score=float(insights_data.get('priority_score', 5.0)),
            confidence_level=insights_data.get('confidence_level', 'medium')
        )
    
    def _generate_action_priority_matrix(self, cluster_insights: List[ClusterInsight]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¡ŒåŠ¨ä¼˜å…ˆçº§çŸ©é˜µ"""
        matrix = []
        
        for insight in cluster_insights:
            for action in insight.recommended_actions:
                matrix.append({
                    "action": action,
                    "cluster_id": insight.cluster_id,
                    "cluster_name": insight.cluster_name,
                    "priority_score": insight.priority_score,
                    "confidence_level": insight.confidence_level,
                    "related_insights": insight.key_insights
                })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        matrix.sort(key=lambda x: x['priority_score'], reverse=True)
        
        return matrix
    
    def _get_default_overall_insights(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ•´ä½“æ´å¯Ÿ"""
        return {
            "overall_sentiment": "neutral",
            "dominant_themes": ["ä¸»é¢˜åˆ†æä¸­"],
            "top_pain_points": ["ç—›ç‚¹åˆ†æä¸­"],
            "key_opportunities": ["æœºä¼šåˆ†æä¸­"],
            "strategic_recommendations": ["å»ºè®®åˆ†æä¸­"]
        }
    
    def _get_default_cluster_insights_data(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤ç°‡æ´å¯Ÿæ•°æ®"""
        return {
            "cluster_name": "åˆ†æä¸­",
            "key_insights": ["æ´å¯Ÿåˆ†æä¸­"],
            "pain_points": ["ç—›ç‚¹åˆ†æä¸­"],
            "opportunities": ["æœºä¼šåˆ†æä¸­"],
            "recommended_actions": ["è¡ŒåŠ¨åˆ†æä¸­"],
            "priority_score": 5.0,
            "confidence_level": "medium"
        }
    
    def _get_default_cluster_insight(self, cluster: Any) -> ClusterInsight:
        """è·å–é»˜è®¤ç°‡æ´å¯Ÿ"""
        return ClusterInsight(
            cluster_id=cluster.cluster_id,
            cluster_name=f'ç°‡ {cluster.cluster_id}',
            key_insights=[f"åŸºäºå…³é”®è¯ {', '.join(cluster.keywords[:3])} çš„è®¨è®º"],
            pain_points=["éœ€è¦è¿›ä¸€æ­¥åˆ†æ"],
            opportunities=["éœ€è¦è¿›ä¸€æ­¥åˆ†æ"],
            recommended_actions=["éœ€è¦è¿›ä¸€æ­¥åˆ†æ"],
            priority_score=5.0,
            confidence_level="low"
        )
    
    def export_insights_to_json(self, business_insight: BusinessInsight, file_path: str) -> bool:
        """å¯¼å‡ºæ´å¯Ÿä¸ºJSONæ–‡ä»¶"""
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            insights_dict = {
                "analysis_timestamp": business_insight.analysis_timestamp.isoformat(),
                "total_clusters": business_insight.total_clusters,
                "total_samples": business_insight.total_samples,
                "overall_sentiment": business_insight.overall_sentiment,
                "dominant_themes": business_insight.dominant_themes,
                "top_pain_points": business_insight.top_pain_points,
                "key_opportunities": business_insight.key_opportunities,
                "strategic_recommendations": business_insight.strategic_recommendations,
                "cluster_insights": [
                    {
                        "cluster_id": insight.cluster_id,
                        "cluster_name": insight.cluster_name,
                        "key_insights": insight.key_insights,
                        "pain_points": insight.pain_points,
                        "opportunities": insight.opportunities,
                        "recommended_actions": insight.recommended_actions,
                        "priority_score": insight.priority_score,
                        "confidence_level": insight.confidence_level
                    }
                    for insight in business_insight.cluster_insights
                ],
                "action_priority_matrix": business_insight.action_priority_matrix
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(insights_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ´å¯ŸæŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ´å¯Ÿå¯¼å‡ºå¤±è´¥: {str(e)}")
            return False
    
    def generate_readable_report(self, business_insight: BusinessInsight) -> str:
        """ç”Ÿæˆå¯è¯»çš„æ´å¯ŸæŠ¥å‘Šæ–‡æœ¬"""
        try:
            report = []
            
            # æŠ¥å‘Šæ ‡é¢˜
            report.append("=" * 80)
            report.append("ğŸ” RedInsight é«˜çº§åˆ†ææ´å¯ŸæŠ¥å‘Š")
            report.append("=" * 80)
            report.append("")
            
            # åˆ†ææ¦‚è§ˆ
            report.append("ğŸ“Š åˆ†ææ¦‚è§ˆ")
            report.append("-" * 40)
            report.append(f"åˆ†ææ—¶é—´: {business_insight.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
            report.append(f"æ€»ç°‡æ•°: {business_insight.total_clusters}")
            report.append(f"æ€»æ ·æœ¬æ•°: {business_insight.total_samples}")
            report.append(f"æ•´ä½“æƒ…æ„Ÿå€¾å‘: {business_insight.overall_sentiment}")
            report.append("")
            
            # ä¸»å¯¼ä¸»é¢˜
            if business_insight.dominant_themes:
                report.append("ğŸ¯ ä¸»å¯¼ä¸»é¢˜")
                report.append("-" * 40)
                for i, theme in enumerate(business_insight.dominant_themes, 1):
                    report.append(f"{i}. {theme}")
                report.append("")
            
            # ä¸»è¦ç—›ç‚¹
            if business_insight.top_pain_points:
                report.append("âš ï¸ ä¸»è¦ç—›ç‚¹")
                report.append("-" * 40)
                for i, pain in enumerate(business_insight.top_pain_points, 1):
                    report.append(f"{i}. {pain}")
                report.append("")
            
            # å…³é”®æœºä¼š
            if business_insight.key_opportunities:
                report.append("ğŸ’¡ å…³é”®æœºä¼š")
                report.append("-" * 40)
                for i, opportunity in enumerate(business_insight.key_opportunities, 1):
                    report.append(f"{i}. {opportunity}")
                report.append("")
            
            # æˆ˜ç•¥å»ºè®®
            if business_insight.strategic_recommendations:
                report.append("ğŸš€ æˆ˜ç•¥å»ºè®®")
                report.append("-" * 40)
                for i, recommendation in enumerate(business_insight.strategic_recommendations, 1):
                    report.append(f"{i}. {recommendation}")
                report.append("")
            
            # å„ç°‡è¯¦ç»†æ´å¯Ÿ
            if business_insight.cluster_insights:
                report.append("ğŸ”¬ å„ç°‡è¯¦ç»†æ´å¯Ÿ")
                report.append("=" * 80)
                
                for insight in business_insight.cluster_insights:
                    report.append(f"\nğŸ“‹ ç°‡ {insight.cluster_id}: {insight.cluster_name}")
                    report.append("-" * 60)
                    report.append(f"ä¼˜å…ˆçº§: {insight.priority_score:.1f}/10")
                    report.append(f"ç½®ä¿¡åº¦: {insight.confidence_level}")
                    report.append("")
                    
                    # å…³é”®æ´å¯Ÿ
                    if insight.key_insights:
                        report.append("ğŸ” å…³é”®æ´å¯Ÿ:")
                        for i, insight_text in enumerate(insight.key_insights, 1):
                            report.append(f"  {i}. {insight_text}")
                        report.append("")
                    
                    # ç—›ç‚¹åˆ†æ
                    if insight.pain_points:
                        report.append("âš ï¸ ç—›ç‚¹åˆ†æ:")
                        for i, pain in enumerate(insight.pain_points, 1):
                            report.append(f"  {i}. {pain}")
                        report.append("")
                    
                    # æœºä¼šè¯†åˆ«
                    if insight.opportunities:
                        report.append("ğŸ’¡ æœºä¼šè¯†åˆ«:")
                        for i, opportunity in enumerate(insight.opportunities, 1):
                            report.append(f"  {i}. {opportunity}")
                        report.append("")
                    
                    # å»ºè®®è¡ŒåŠ¨
                    if insight.recommended_actions:
                        report.append("ğŸ¯ å»ºè®®è¡ŒåŠ¨:")
                        for i, action in enumerate(insight.recommended_actions, 1):
                            report.append(f"  {i}. {action}")
                        report.append("")
            
            # è¡ŒåŠ¨ä¼˜å…ˆçº§çŸ©é˜µ
            if business_insight.action_priority_matrix:
                report.append("ğŸ“ˆ è¡ŒåŠ¨ä¼˜å…ˆçº§çŸ©é˜µ")
                report.append("=" * 80)
                report.append("æŒ‰ä¼˜å…ˆçº§æ’åºçš„è¡ŒåŠ¨å»ºè®®:")
                report.append("")
                
                for i, action_item in enumerate(business_insight.action_priority_matrix[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
                    report.append(f"{i}. {action_item['action']}")
                    report.append(f"   æ¥æº: {action_item['cluster_name']} (ç°‡ {action_item['cluster_id']})")
                    report.append(f"   ä¼˜å…ˆçº§: {action_item['priority_score']:.1f}/10")
                    report.append(f"   ç½®ä¿¡åº¦: {action_item['confidence_level']}")
                    report.append("")
            
            # æŠ¥å‘Šç»“å°¾
            report.append("=" * 80)
            report.append("ğŸ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            report.append(f"ç”Ÿæˆæ—¶é—´: {business_insight.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
            report.append("=" * 80)
            
            return "\n".join(report)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è¯»æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def export_insights_to_text(self, business_insight: BusinessInsight, file_path: str) -> bool:
        """å¯¼å‡ºæ´å¯Ÿä¸ºå¯è¯»æ–‡æœ¬æ–‡ä»¶"""
        try:
            # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
            readable_report = self.generate_readable_report(business_insight)
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(readable_report)
            
            logger.info(f"å¯è¯»æ´å¯ŸæŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"å¯è¯»æŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {str(e)}")
            return False
