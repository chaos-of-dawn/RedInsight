"""
æ•°æ®åˆ†æä¸ç»“æœå±•ç¤ºé¡µé¢
åŒ…å«æ•°æ®ç®¡ç†ã€æ•°æ®æ•´ç†å’Œæœ¬åœ°ç­›é€‰åŠŸèƒ½
"""
import streamlit as st
import json
import time
import hashlib
from datetime import datetime, timedelta
import logging
from database import DatabaseManager
from llm_analyzer import LLMAnalyzer
from data_organizer import DataOrganizer

def create_unique_key(prefix, group_key, suffix=""):
    """åˆ›å»ºå”¯ä¸€çš„keyï¼Œé¿å…å†²çª"""
    # ä½¿ç”¨group_keyçš„hashå€¼æ¥ç¡®ä¿å”¯ä¸€æ€§
    key_hash = hashlib.md5(f"{group_key}_{suffix}".encode()).hexdigest()[:8]
    return f"{prefix}_{key_hash}"

def create_state_key(prefix, group_key):
    """åˆ›å»ºå”¯ä¸€çš„çŠ¶æ€é”®"""
    return f"{prefix}_{group_key}"

def get_state_value(key, default=False):
    """å®‰å…¨åœ°è·å–çŠ¶æ€å€¼"""
    return st.session_state.get(key, default)

def toggle_state(key, value):
    """åˆ‡æ¢çŠ¶æ€å€¼å¹¶é‡æ–°è¿è¡Œ"""
    st.session_state[key] = value
    st.rerun()

def create_merged_analysis_page():
    """åˆ›å»ºåˆå¹¶çš„æ•°æ®åˆ†æä¸ç»“æœå±•ç¤ºé¡µé¢"""
    
    st.header("ğŸ“Š æœ¬åœ°æ•°æ®ç®¡ç†")
    
    # æ£€æŸ¥åå°åˆ†æçŠ¶æ€
    try:
        from background_analyzer import background_analyzer
        analysis_status = background_analyzer.get_status()
        
        if analysis_status.get('running', False):
            st.info("ğŸ”„ åå°åˆ†ææ­£åœ¨è¿›è¡Œä¸­ï¼Œæ‚¨å¯ä»¥æ­£å¸¸ä½¿ç”¨æ•°æ®ç®¡ç†åŠŸèƒ½")
            st.info(f"åˆ†æçŠ¶æ€: {analysis_status.get('status', 'æœªçŸ¥çŠ¶æ€')}")
    except ImportError:
        pass
    
    if not st.session_state.initialized:
        st.warning("è¯·å…ˆé…ç½®APIå¯†é’¥å¹¶åˆå§‹åŒ–ç³»ç»Ÿ")
        return
    
    # æ˜¾ç¤ºé¡µé¢è¯´æ˜
    st.info("ğŸ’¡ æ­¤é¡µé¢æä¾›æ•°æ®ç®¡ç†ã€æ•´ç†å’Œæœ¬åœ°ç­›é€‰åŠŸèƒ½")
    
    # åˆ›å»ºä¾§è¾¹æ ç”¨äºå¯¼èˆª
    st.sidebar.markdown("### ğŸ¯ åŠŸèƒ½å¯¼èˆª")
    
    # åˆå§‹åŒ–é¡µé¢é€‰é¡¹
    if 'page_option' not in st.session_state:
        st.session_state.page_option = "ğŸ“‹ æ•°æ®ç®¡ç†"
    
    page_option = st.sidebar.selectbox(
        "é€‰æ‹©åŠŸèƒ½æ¨¡å—",
        ["ğŸ“‹ æ•°æ®ç®¡ç†", "ğŸ“¦ æ•°æ®æ•´ç†æ‰“åŒ…", "ğŸ” æ•°æ®ç­›é€‰", "ğŸ“Š ç»“æœå±•ç¤º"],
        index=["ğŸ“‹ æ•°æ®ç®¡ç†", "ğŸ“¦ æ•°æ®æ•´ç†æ‰“åŒ…", "ğŸ” æ•°æ®ç­›é€‰", "ğŸ“Š ç»“æœå±•ç¤º"].index(st.session_state.page_option) if st.session_state.page_option in ["ğŸ“‹ æ•°æ®ç®¡ç†", "ğŸ“¦ æ•°æ®æ•´ç†æ‰“åŒ…", "ğŸ” æ•°æ®ç­›é€‰", "ğŸ“Š ç»“æœå±•ç¤º"] else 0,
        help="é€‰æ‹©è¦ä½¿ç”¨çš„åŠŸèƒ½æ¨¡å—"
    )
    
    # æ›´æ–°é¡µé¢é€‰é¡¹
    if page_option != st.session_state.page_option:
        st.session_state.page_option = page_option
    
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºå½“å‰é€‰ä¸­çš„æ¨¡å—
    st.sidebar.markdown(f"**å½“å‰æ¨¡å—:** {page_option}")
    
    # æ ¹æ®é€‰æ‹©çš„æ¨¡å—æ˜¾ç¤ºå¯¹åº”åŠŸèƒ½
    if st.session_state.page_option == "ğŸ“‹ æ•°æ®ç®¡ç†":
        show_data_management()
    elif st.session_state.page_option == "ğŸ“¦ æ•°æ®æ•´ç†æ‰“åŒ…":
        show_data_packaging()
    elif st.session_state.page_option == "ğŸ” æ•°æ®ç­›é€‰":
        show_data_filtering()
    elif st.session_state.page_option == "ğŸ“Š ç»“æœå±•ç¤º":
        show_results_display()

def show_data_filtering():
    """æ˜¾ç¤ºæ•°æ®ç­›é€‰åŠŸèƒ½"""
    st.subheader("ğŸ” æ•°æ®ç­›é€‰")
    st.write("æ ¹æ®æ—¥æœŸç­‰æ¡ä»¶ç­›é€‰å¸–å­æ•°æ®")
    
    # æ—¥æœŸç­›é€‰
    st.markdown("#### ğŸ“… æ—¥æœŸç­›é€‰")
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=None, help="ç­›é€‰æ­¤æ—¥æœŸä¹‹åçš„å¸–å­")
    with col2:
        end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=None, help="ç­›é€‰æ­¤æ—¥æœŸä¹‹å‰çš„å¸–å­")
    
    # å­ç‰ˆå—ç­›é€‰
    st.markdown("#### ğŸ·ï¸ å­ç‰ˆå—ç­›é€‰")
    try:
        available_subreddits = st.session_state.db.get_subreddit_list()
        if available_subreddits:
            selected_subreddits = st.multiselect(
                "é€‰æ‹©å­ç‰ˆå—",
                options=available_subreddits,
                default=available_subreddits,
                help="é€‰æ‹©è¦ç­›é€‰çš„å­ç‰ˆå—"
            )
        else:
            st.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å­ç‰ˆå—æ•°æ®")
            selected_subreddits = []
    except Exception as e:
        st.error(f"è·å–å­ç‰ˆå—åˆ—è¡¨å¤±è´¥: {str(e)}")
        selected_subreddits = []
    
    # åº”ç”¨ç­›é€‰
    if st.button("ğŸ” åº”ç”¨ç­›é€‰", type="primary"):
        try:
            # è·å–æ‰€æœ‰å¸–å­æ•°æ®
            all_posts = st.session_state.db.get_posts_with_analysis(limit=10000)
            
            if not all_posts:
                st.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å¸–å­æ•°æ®")
                return
            
            # åº”ç”¨ç­›é€‰æ¡ä»¶
            filtered_posts = []
            for post_data in all_posts:
                post = post_data['post']
                
                # å­ç‰ˆå—ç­›é€‰
                if selected_subreddits and post.subreddit not in selected_subreddits:
                    continue
                
                # æ—¥æœŸç­›é€‰
                if start_date and post.created_utc.date() < start_date:
                    continue
                if end_date and post.created_utc.date() > end_date:
                    continue
                
                filtered_posts.append(post_data)
            
            # æ˜¾ç¤ºç­›é€‰ç»“æœ
            st.success(f"âœ… ç­›é€‰å®Œæˆï¼ä» {len(all_posts)} ä¸ªå¸–å­ä¸­ç­›é€‰å‡º {len(filtered_posts)} ä¸ªç¬¦åˆæ¡ä»¶çš„å¸–å­")
            
            # æ˜¾ç¤ºç­›é€‰ç»Ÿè®¡
            if filtered_posts:
                st.markdown("#### ğŸ“ˆ ç­›é€‰ç»Ÿè®¡")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ç­›é€‰åå¸–å­æ•°", len(filtered_posts))
                with col2:
                    avg_score = sum(post_data['post'].score for post_data in filtered_posts) / len(filtered_posts)
                    st.metric("å¹³å‡åˆ†æ•°", f"{avg_score:.1f}")
                with col3:
                    subreddit_counts = {}
                    for post_data in filtered_posts:
                        subreddit = post_data['post'].subreddit
                        subreddit_counts[subreddit] = subreddit_counts.get(subreddit, 0) + 1
                    st.metric("æ¶‰åŠå­ç‰ˆå—", len(subreddit_counts))
                
                # æ˜¾ç¤ºç­›é€‰åçš„æ•°æ®
                st.markdown("#### ğŸ“‹ ç­›é€‰ç»“æœ")
                for i, post_data in enumerate(filtered_posts[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                    post = post_data['post']
                    with st.expander(f"{i+1}. {post.title[:50]}... (r/{post.subreddit}, åˆ†æ•°: {post.score})"):
                        st.write(f"**ä½œè€…**: {post.author}")
                        st.write(f"**åˆ†æ•°**: {post.score}")
                        st.write(f"**è¯„è®ºæ•°**: {post.num_comments}")
                        st.write(f"**å‘å¸ƒæ—¶é—´**: {post.created_utc}")
                        st.write(f"**å†…å®¹**: {post.selftext[:200]}..." if post.selftext else "æ— å†…å®¹")
            
            # ä¿å­˜ç­›é€‰ç»“æœåˆ°session state
            st.session_state.filtered_posts = filtered_posts
            
        except Exception as e:
            st.error(f"ç­›é€‰å¤±è´¥: {str(e)}")

def show_data_management():
    """æ˜¾ç¤ºæ•°æ®ç®¡ç†åŠŸèƒ½"""
    st.subheader("ğŸ—‚ï¸ æ•°æ®ç®¡ç†")
    
    # æ•°æ®ç»Ÿè®¡
    try:
        stats = st.session_state.db.get_analysis_statistics()
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("æ€»å¸–å­", stats.get('total_posts', 0))
        with col2:
            st.metric("æ€»è¯„è®º", stats.get('total_comments', 0))
        with col3:
            st.metric("æƒ…æ„Ÿåˆ†æ", stats.get('sentiment_count', 0))
        with col4:
            st.metric("ä¸»é¢˜åˆ†æ", stats.get('topic_count', 0))
        with col5:
            st.metric("è´¨é‡è¯„ä¼°", stats.get('quality_count', 0))
        
    except Exception as e:
        st.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    # æ•°æ®ç®¡ç†æ“ä½œ
    st.subheader("æ•°æ®æ“ä½œ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", type="secondary"):
            if st.session_state.db.clear_all_data():
                st.success("âœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç©º")
                st.rerun()
            else:
                st.error("âŒ æ¸…ç©ºæ•°æ®å¤±è´¥")
    
    with col2:
        # è·å–å­ç‰ˆå—åˆ—è¡¨
        subreddits_list = st.session_state.db.get_subreddit_list()
        if subreddits_list:
            selected_subreddit = st.selectbox("é€‰æ‹©å­ç‰ˆå—", ["å…¨éƒ¨"] + subreddits_list, key="data_management_subreddit")
        else:
            selected_subreddit = "å…¨éƒ¨"
            st.info("æš‚æ— å­ç‰ˆå—æ•°æ®")
    
    with col3:
        batch_limit = st.number_input("æ‰¹é‡åˆ†ææ•°é‡", min_value=5, max_value=100, value=25, key="batch_analysis_limit")
    
    with col4:
        batch_ai_provider = st.selectbox("æ‰¹é‡åˆ†æAIæä¾›å•†", ["openai", "anthropic", "deepseek"], key="batch_ai_provider")
    
    # æ•°æ®åˆ†ç»„ç®¡ç†
    st.subheader("ğŸ“‹ æ•°æ®åˆ†ç»„ç®¡ç†")
    st.write("æŒ‰æœç´¢æ—¥æœŸå’Œæ¿å—åç§°åˆ†ç»„æ˜¾ç¤ºæ•°æ®ï¼Œæ”¯æŒæ‰¹é‡æ“ä½œ")
    
    try:
        # è·å–åˆ†ç»„æ•°æ®
        grouped_data = st.session_state.db.get_posts_grouped_by_date_subreddit()
        
        if grouped_data:
            # æ˜¾ç¤ºåˆ†ç»„åˆ—è¡¨
            for group_key, group_info in grouped_data.items():
                date = group_info['date']
                subreddit = group_info['subreddit']
                total_posts = group_info['total_posts']
                total_comments = group_info['total_comments']
                
                # åˆ›å»ºåˆ†ç»„æ ‡é¢˜
                group_title = f"ğŸ“… {date} | ğŸ“ r/{subreddit} | ğŸ“Š {total_posts}ä¸ªå¸–å­ | ğŸ’¬ {total_comments}æ¡è¯„è®º"
                
                with st.expander(group_title, expanded=False):
                    # åˆ†ç»„æ“ä½œæŒ‰é’®
                    col_op1, col_op2, col_op3, col_op4 = st.columns(4)
                    
                    with col_op1:
                        show_details = st.checkbox("ğŸ‘€ æŸ¥çœ‹è¯¦æƒ…", key=create_unique_key("view", group_key))
                    
                    with col_op2:
                        show_package = st.checkbox("ğŸ“¦ æ•´ç†æ‰“åŒ…", key=create_unique_key("package", group_key))
                    
                    with col_op3:
                        show_llm = st.checkbox("ğŸ¤– ä¼ é€’ç»™å¤§æ¨¡å‹", key=create_unique_key("llm_process", group_key))
                    
                    with col_op4:
                        if st.button("ğŸ—‘ï¸ åˆ é™¤åˆ†ç»„", key=create_unique_key("delete", group_key), type="secondary"):
                            if st.session_state.db.delete_posts_by_group(date, subreddit):
                                st.success(f"âœ… å·²åˆ é™¤ {date} r/{subreddit} çš„æ‰€æœ‰æ•°æ®")
                                st.rerun()
                            else:
                                st.error("âŒ åˆ é™¤å¤±è´¥")
                    
                    # æ˜¾ç¤ºè¯¦æƒ…
                    if show_details:
                        st.write("**å¸–å­è¯¦æƒ…:**")
                        posts = group_info['posts']
                        
                        # å¸–å­åˆ—è¡¨
                        for i, post in enumerate(posts[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                            # ä½¿ç”¨å®¹å™¨è€Œä¸æ˜¯expanderæ¥é¿å…åµŒå¥—
                            with st.container():
                                st.write(f"**ğŸ“Œ {post.title[:60]}... (åˆ†æ•°: {post.score})**")
                                col_detail1, col_detail2 = st.columns([3, 1])
                                
                                with col_detail1:
                                    st.write(f"**ä½œè€…:** u/{post.author}")
                                    st.write(f"**åˆ†æ•°:** {post.score} | **è¯„è®ºæ•°:** {post.num_comments}")
                                    st.write(f"**æ—¶é—´:** {post.created_utc.strftime('%Y-%m-%d %H:%M')}")
                                    if post.selftext:
                                        st.write(f"**å†…å®¹:** {post.selftext[:200]}...")
                                
                                with col_detail2:
                                    if st.button("ğŸ—‘ï¸ åˆ é™¤", key=f"delete_post_{post.id}", type="secondary"):
                                        if st.session_state.db.delete_post(post.id):
                                            st.success("âœ… å¸–å­å·²åˆ é™¤")
                                            st.rerun()
                                        else:
                                            st.error("âŒ åˆ é™¤å¤±è´¥")
                                
                                st.divider()  # æ·»åŠ åˆ†éš”çº¿
                        
                        if len(posts) > 10:
                            st.info(f"è¿˜æœ‰ {len(posts) - 10} ä¸ªå¸–å­æœªæ˜¾ç¤º...")
                    
                    # æ•´ç†æ‰“åŒ…
                    if show_package:
                        show_group_packaging(group_key, group_info)
                    
                    # ä¼ é€’ç»™å¤§æ¨¡å‹
                    if show_llm:
                        show_group_llm_processing(group_key, group_info)
        else:
            st.info("æš‚æ— åˆ†ç»„æ•°æ®")
            
    except Exception as e:
        st.error(f"è·å–åˆ†ç»„æ•°æ®å¤±è´¥: {str(e)}")

def show_group_packaging(group_key, group_info):
    """æ˜¾ç¤ºåˆ†ç»„æ•°æ®æ‰“åŒ…åŠŸèƒ½"""
    st.write("**æ•°æ®æ‰“åŒ…é€‰é¡¹:**")
    
    col_pack1, col_pack2 = st.columns(2)
    
    with col_pack1:
        output_format = st.selectbox("è¾“å‡ºæ ¼å¼", ["JSON", "TXT", "Markdown"], key=create_unique_key("format", group_key))
        include_metadata = st.checkbox("åŒ…å«å…ƒæ•°æ®", value=True, key=create_unique_key("meta", group_key))
    
    with col_pack2:
        use_llm_summary = st.checkbox("ä½¿ç”¨LLMç”Ÿæˆç²¾å‡†æ¢—æ¦‚", value=False, key=create_unique_key("llm_summary", group_key))
        ai_provider = st.selectbox("AIæä¾›å•†", ["openai", "anthropic", "deepseek"], key=create_unique_key("provider", group_key))
    
    if st.button("ğŸ“¦ å¼€å§‹æ‰“åŒ…", key=create_unique_key("start_package", group_key)):
        try:
            from data_organizer import DataOrganizer
            
            # åˆå§‹åŒ–LLMåˆ†æå™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            llm_analyzer = None
            if use_llm_summary and st.session_state.analyzer:
                llm_analyzer = st.session_state.analyzer
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            organizer = DataOrganizer(st.session_state.db, llm_analyzer)
            
            # æ•´ç†è¯¥åˆ†ç»„çš„æ•°æ®
            organized_data = {
                "metadata": {
                    "total_groups": 1,
                    "total_posts": group_info['total_posts'],
                    "total_comments": group_info['total_comments'],
                    "date_range": {"start": group_info['date'], "end": group_info['date']},
                    "subreddits": [group_info['subreddit']],
                    "organized_at": datetime.now().isoformat()
                },
                "scraping_sessions": [{
                    "session_id": group_key,
                    "scraping_date": group_info['date'],
                    "subreddit": group_info['subreddit'],
                    "statistics": {
                        "total_posts": group_info['total_posts'],
                        "total_comments": group_info['total_comments'],
                        "avg_score": sum(post.score or 0 for post in group_info['posts']) / len(group_info['posts']) if group_info['posts'] else 0,
                        "top_author": max([post.author for post in group_info['posts'] if post.author], key=[post.author for post in group_info['posts'] if post.author].count) if group_info['posts'] else "æ— "
                    },
                    "content_summary": "å¾…ç”Ÿæˆ",
                    "key_topics": [],
                    "sentiment_overview": {"positive": 0, "negative": 0, "neutral": 0},
                    "posts": [{
                        "id": post.id,
                        "title": post.title,
                        "author": post.author,
                        "score": post.score,
                        "num_comments": post.num_comments,
                        "created_utc": post.created_utc.isoformat() if post.created_utc else None,
                        "content": post.selftext,
                        "url": post.url,
                        "flair": post.flair,
                        "is_self": post.is_self,
                        "over_18": post.over_18
                    } for post in group_info['posts']]
                }]
            }
            
            # åˆ›å»ºæ•°æ®åŒ…
            package_content = organizer.create_llm_ready_package(
                organized_data,
                output_format=output_format.lower(),
                include_metadata=include_metadata
            )
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"reddit_data_{group_key}_{timestamp}.{output_format.lower()}"
            
            # ä¿å­˜æ–‡ä»¶
            file_path = organizer.save_package_to_file(
                package_content,
                filename,
                "output"
            )
            
            st.success(f"âœ… æ•°æ®åŒ…å·²ä¿å­˜åˆ°: {file_path}")
            
            # æä¾›ä¸‹è½½æŒ‰é’®
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ•°æ®åŒ…",
                data=package_content,
                file_name=filename,
                mime="application/octet-stream"
            )
            
        except Exception as e:
            st.error(f"âŒ æ•°æ®æ‰“åŒ…å¤±è´¥: {str(e)}")

def show_group_llm_processing(group_key, group_info):
    """æ˜¾ç¤ºåˆ†ç»„æ•°æ®ä¼ é€’ç»™å¤§æ¨¡å‹å¤„ç†"""
    st.write("**å¤§æ¨¡å‹å¤„ç†é€‰é¡¹:**")
    
    # é€‰æ‹©å¤„ç†è§„åˆ™
    processing_rules = st.session_state.get('llm_processing_rules', {})
    
    if not processing_rules:
        st.warning("è¯·å…ˆåœ¨å¤§æ¨¡å‹å¤„ç†è§„åˆ™é¡µé¢è®¾ç½®å¤„ç†è§„åˆ™")
        return
    
    col_llm1, col_llm2 = st.columns(2)
    
    with col_llm1:
        selected_rule = st.selectbox(
            "é€‰æ‹©å¤„ç†è§„åˆ™",
            list(processing_rules.keys()),
            key=create_unique_key("rule", group_key)
        )
        
        ai_provider = st.selectbox(
            "AIæä¾›å•†",
            ["openai", "anthropic", "deepseek"],
            key=create_unique_key("llm_provider", group_key)
        )
    
    with col_llm2:
        batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", min_value=1, max_value=100, value=25, key=create_unique_key("batch", group_key))
        use_custom_prompt = st.checkbox("ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯", value=False, key=create_unique_key("custom", group_key))
    
    if use_custom_prompt:
        custom_prompt = st.text_area(
            "è‡ªå®šä¹‰æç¤ºè¯",
            value=processing_rules[selected_rule]['prompt_template'],
            height=200,
            key=create_unique_key("prompt", group_key)
        )
    else:
        custom_prompt = processing_rules[selected_rule]['prompt_template']
    
    if st.button("ğŸ¤– å¼€å§‹å¤§æ¨¡å‹å¤„ç†", key=create_unique_key("start_llm", group_key)):
        try:
            # å‡†å¤‡æ•°æ®
            posts_data = []
            for post in group_info['posts'][:batch_size]:
                posts_data.append({
                    'id': post.id,
                    'title': post.title,
                    'content': post.selftext or "",
                    'author': post.author,
                    'score': post.score,
                    'subreddit': post.subreddit
                })
            
            # æ‰§è¡Œå¤§æ¨¡å‹å¤„ç†
            if processing_rules[selected_rule]['analysis_type'] == 'comprehensive':
                result = st.session_state.analyzer.analyze_comprehensive(
                    "\n".join([f"æ ‡é¢˜: {p['title']}\nå†…å®¹: {p['content']}" for p in posts_data]),
                    ai_provider,
                    custom_prompt
                )
            else:
                result = st.session_state.analyzer.analyze_posts_batch(
                    posts_data,
                    ai_provider,
                    processing_rules[selected_rule]['analysis_type']
                )
            
            if "error" not in result:
                # ä¿å­˜å¤„ç†ç»“æœ
                st.session_state.db.save_analysis_result(
                    f"manual_{group_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    "manual", processing_rules[selected_rule]['analysis_type'],
                    str(result), ai_provider
                )
                
                st.success(f"âœ… å¤§æ¨¡å‹å¤„ç†å®Œæˆï¼å¤„ç†äº† {len(posts_data)} ä¸ªå¸–å­")
                
                # æ˜¾ç¤ºç»“æœ
                with st.expander("ğŸ“Š å¤„ç†ç»“æœ"):
                    st.json(result)
            else:
                st.error(f"âŒ å¤§æ¨¡å‹å¤„ç†å¤±è´¥: {result['error']}")
                
        except Exception as e:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")

def show_data_packaging():
    """æ˜¾ç¤ºæ•°æ®æ•´ç†æ‰“åŒ…åŠŸèƒ½"""
    st.subheader("ğŸ“¦ æ•°æ®æ•´ç†å’Œæ‰“åŒ…")
    st.write("å°†æŠ“å–çš„æ•°æ®æ•´ç†æˆå¤§æ¨¡å‹å¯ç†è§£çš„æ ¼å¼ï¼Œæ”¯æŒæŒ‰æ¡ä»¶ç­›é€‰å’Œå¤šç§è¾“å‡ºæ ¼å¼")
    
    col_package1, col_package2 = st.columns(2)
    
    with col_package1:
        st.write("**æ•°æ®ç­›é€‰æ¡ä»¶**")
        
        # æ—¥æœŸèŒƒå›´é€‰æ‹©
        date_range = st.date_input(
            "é€‰æ‹©æ—¥æœŸèŒƒå›´",
            value=(datetime.now() - timedelta(days=7), datetime.now()),
            help="é€‰æ‹©è¦æ•´ç†çš„æ•°æ®æ—¥æœŸèŒƒå›´"
        )
        
        # å­ç‰ˆå—é€‰æ‹©
        available_subreddits = st.session_state.db.get_subreddit_list()
        selected_subreddits = st.multiselect(
            "é€‰æ‹©å­ç‰ˆå—",
            options=available_subreddits,
            default=available_subreddits[:3] if available_subreddits else [],
            help="é€‰æ‹©è¦åŒ…å«çš„å­ç‰ˆå—ï¼Œç•™ç©ºè¡¨ç¤ºå…¨éƒ¨"
        )
    
    with col_package2:
        st.write("**è¾“å‡ºè®¾ç½®**")
        
        # è¾“å‡ºæ ¼å¼
        output_format = st.selectbox(
            "è¾“å‡ºæ ¼å¼",
            ["JSON", "TXT", "Markdown"],
            help="é€‰æ‹©æ•°æ®åŒ…çš„è¾“å‡ºæ ¼å¼"
        )
        
        # æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        include_metadata = st.checkbox(
            "åŒ…å«å…ƒæ•°æ®",
            value=True,
            help="æ˜¯å¦åœ¨æ•°æ®åŒ…ä¸­åŒ…å«ç»Ÿè®¡ä¿¡æ¯å’Œå…ƒæ•°æ®",
            key="global_include_metadata"
        )
        
        # æ˜¯å¦ä½¿ç”¨LLMç”Ÿæˆæ¢—æ¦‚
        use_llm_summary = st.checkbox(
            "ä½¿ç”¨LLMç”Ÿæˆç²¾å‡†æ¢—æ¦‚",
            value=False,
            help="ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ›´ç²¾å‡†çš„å†…å®¹æ¢—æ¦‚ï¼ˆéœ€è¦é…ç½®APIå¯†é’¥ï¼‰",
            key="global_use_llm_summary"
        )
        
        # æ˜¯å¦ç›´æ¥ä¼ é€’ç»™å¤§æ¨¡å‹åˆ†æ
        direct_llm_analysis = st.checkbox(
            "ç›´æ¥ä¼ é€’ç»™å¤§æ¨¡å‹åˆ†æ",
            value=False,
            help="æ•´ç†æ‰“åŒ…åç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ",
            key="global_direct_llm_analysis"
        )
    
    # æ˜¾ç¤ºå¤„ç†è§„åˆ™é€‰æ‹©ï¼ˆæ— è®ºæ˜¯å¦é€‰æ‹©ç›´æ¥åˆ†æï¼‰
    st.write("**ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™é€‰æ‹©**")
    
    processing_rules = st.session_state.get('llm_processing_rules', {})
    if not processing_rules:
        st.warning("âš ï¸ è¯·å…ˆè®¾ç½®å¤„ç†è§„åˆ™")
        if st.button("ğŸš€ ç«‹å³è®¾ç½®å¤„ç†è§„åˆ™", type="primary"):
            st.session_state.page_option = "ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™"
            st.rerun()
    else:
        col_rule1, col_rule2 = st.columns(2)
        
        with col_rule1:
            selected_rule = st.selectbox(
                "é€‰æ‹©å¤„ç†è§„åˆ™",
                list(processing_rules.keys()),
                key="package_selected_rule",
                help="é€‰æ‹©è¦ä½¿ç”¨çš„å¤„ç†è§„åˆ™"
            )
            
            ai_provider = st.selectbox(
                "AIæä¾›å•†",
                ["openai", "anthropic", "deepseek"],
                key="package_ai_provider",
                help="é€‰æ‹©AIæä¾›å•†"
            )
        
        with col_rule2:
            batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", min_value=1, max_value=100, value=25, key="package_batch_size", help="æ¯æ‰¹å¤„ç†çš„å¸–å­æ•°é‡")
            use_custom_prompt = st.checkbox("ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯", value=False, key="package_use_custom_prompt", help="æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯")
        
        if use_custom_prompt:
            custom_prompt = st.text_area(
                "è‡ªå®šä¹‰æç¤ºè¯",
                value=processing_rules[selected_rule]['prompt_template'],
                height=200,
                key="package_custom_prompt",
                help="è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨{text}ä½œä¸ºæ•°æ®å ä½ç¬¦"
            )
        else:
            custom_prompt = processing_rules[selected_rule]['prompt_template']
        
        # æ˜¾ç¤ºé€‰ä¸­çš„è§„åˆ™ä¿¡æ¯
        st.info(f"ğŸ“‹ å·²é€‰æ‹©è§„åˆ™: **{selected_rule}** | åˆ†æç±»å‹: **{processing_rules[selected_rule]['analysis_type']}** | AIæä¾›å•†: **{ai_provider}**")
    
    # æ•°æ®æ•´ç†å’Œæ‰“åŒ…æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æ•°æ®æ•´ç†å’Œæ‰“åŒ…", type="primary"):
        try:
            from data_organizer import DataOrganizer
            
            # å‡†å¤‡å‚æ•°
            start_date = date_range[0].strftime('%Y-%m-%d') if date_range[0] else None
            end_date = date_range[1].strftime('%Y-%m-%d') if date_range[1] else None
            subreddits = selected_subreddits if selected_subreddits else None
            
            # åˆå§‹åŒ–LLMåˆ†æå™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            llm_analyzer = None
            if use_llm_summary and st.session_state.analyzer:
                llm_analyzer = st.session_state.analyzer
            
            # åˆ›å»ºæ•°æ®æ•´ç†å™¨
            organizer = DataOrganizer(st.session_state.db, llm_analyzer)
            
            # æ˜¾ç¤ºè¿›åº¦
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("æ­£åœ¨æ•´ç†æ•°æ®...")
            progress_bar.progress(20)
            
            # æ•´ç†æ•°æ®
            organized_data = organizer.organize_data_by_scraping_session(
                start_date=start_date,
                end_date=end_date,
                subreddits=subreddits
            )
            
            if "error" in organized_data:
                st.error(f"âŒ æ•°æ®æ•´ç†å¤±è´¥: {organized_data['error']}")
                return
            
            status_text.text("æ­£åœ¨åˆ›å»ºæ•°æ®åŒ…...")
            progress_bar.progress(60)
            
            # åˆ›å»ºæ•°æ®åŒ…
            package_content = organizer.create_llm_ready_package(
                organized_data,
                output_format=output_format.lower(),
                include_metadata=include_metadata
            )
            
            status_text.text("æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
            progress_bar.progress(80)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"reddit_data_{timestamp}.{output_format.lower()}"
            
            # ä¿å­˜æ–‡ä»¶
            file_path = organizer.save_package_to_file(
                package_content,
                filename,
                "output"
            )
            
            progress_bar.progress(100)
            status_text.text("âœ… æ•°æ®æ‰“åŒ…å®Œæˆ!")
            
            # æ˜¾ç¤ºç»“æœ
            metadata = organized_data.get('metadata', {})
            
            st.success("ğŸ‰ æ•°æ®æ•´ç†å’Œæ‰“åŒ…å®Œæˆ!")
            
            col_result1, col_result2 = st.columns(2)
            
            with col_result1:
                st.write("**ğŸ“Š ç»Ÿè®¡ä¿¡æ¯**")
                st.metric("æ€»åˆ†ç»„æ•°", metadata.get('total_groups', 0))
                st.metric("æ€»å¸–å­æ•°", metadata.get('total_posts', 0))
                st.metric("æ€»è¯„è®ºæ•°", metadata.get('total_comments', 0))
                
                if metadata.get('subreddits'):
                    st.write("**ğŸ“ æ¶‰åŠå­ç‰ˆå—**")
                    for subreddit in metadata['subreddits']:
                        st.write(f"- r/{subreddit}")
            
            with col_result2:
                st.write("**ğŸ“ æ–‡ä»¶ä¿¡æ¯**")
                st.write(f"æ–‡ä»¶å: `{filename}`")
                st.write(f"æ ¼å¼: {output_format}")
                st.write(f"å¤§å°: {len(package_content.encode('utf-8')) / 1024:.1f} KB")
                
                # æä¾›ä¸‹è½½æŒ‰é’®
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æ•°æ®åŒ…",
                    data=package_content,
                    file_name=filename,
                    mime="application/octet-stream"
                )
            
            # å¦‚æœé€‰æ‹©ç›´æ¥åˆ†æï¼Œè¿›è¡Œå¤§æ¨¡å‹åˆ†æ
            if direct_llm_analysis and processing_rules:
                st.write("---")
                st.write("**ğŸ¤– å¼€å§‹å¤§æ¨¡å‹åˆ†æ**")
                
                try:
                    # å‡†å¤‡åˆ†ææ•°æ®
                    analysis_data = []
                    for session_key, session_data in organized_data.get('sessions', {}).items():
                        for post in session_data.get('posts', []):
                            analysis_data.append({
                                'title': post.get('title', ''),
                                'content': post.get('content', ''),
                                'author': post.get('author', ''),
                                'score': post.get('score', 0),
                                'subreddit': post.get('subreddit', ''),
                                'created_utc': post.get('created_utc', ''),
                                'num_comments': post.get('num_comments', 0)
                            })
                    
                    if analysis_data:
                        # åˆ†æ‰¹å¤„ç†
                        batch_size = st.session_state.get('package_batch_size', 10)
                        total_batches = (len(analysis_data) + batch_size - 1) // batch_size
                        
                        st.write(f"ğŸ“Š å°†å¤„ç† {len(analysis_data)} æ¡æ•°æ®ï¼Œåˆ†ä¸º {total_batches} æ‰¹")
                        
                        for i in range(0, len(analysis_data), batch_size):
                            batch_data = analysis_data[i:i + batch_size]
                            batch_num = i // batch_size + 1
                            
                            st.write(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹...")
                            
                            # ä½¿ç”¨é€‰å®šçš„è§„åˆ™è¿›è¡Œåˆ†æ
                            if st.session_state.analyzer:
                                result = st.session_state.analyzer.analyze_posts_batch(
                                    batch_data,
                                    provider=st.session_state.get('package_ai_provider', 'openai'),
                                    analysis_type=processing_rules[selected_rule]['analysis_type']
                                )
                                
                                if 'error' not in result:
                                    # ä¿å­˜åˆ†æç»“æœ
                                    st.session_state.db.save_analysis_result(
                                        analysis_type=processing_rules[selected_rule]['analysis_type'],
                                        result=result,
                                        posts_count=len(batch_data)
                                    )
                                    st.success(f"âœ… ç¬¬ {batch_num} æ‰¹åˆ†æå®Œæˆ")
                                else:
                                    st.error(f"âŒ ç¬¬ {batch_num} æ‰¹åˆ†æå¤±è´¥: {result['error']}")
                            else:
                                st.error("âŒ å¤§æ¨¡å‹åˆ†æå™¨æœªåˆå§‹åŒ–")
                                break
                        
                        st.success("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡åˆ†æå®Œæˆï¼")
                    else:
                        st.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
                        
                except Exception as e:
                    st.error(f"âŒ å¤§æ¨¡å‹åˆ†æå¤±è´¥: {str(e)}")
                    import traceback
                    st.error(traceback.format_exc())
            
            # æ˜¾ç¤ºæ•°æ®åŒ…é¢„è§ˆ
            with st.expander("ğŸ‘€ æ•°æ®åŒ…é¢„è§ˆ"):
                if output_format.lower() == "json":
                    st.json(organized_data)
                else:
                    # æ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦
                    preview = package_content[:1000]
                    if len(package_content) > 1000:
                        preview += "\n\n... (æ•°æ®åŒ…å†…å®¹è¾ƒé•¿ï¼Œå·²æˆªæ–­)"
                    st.text(preview)
            
        except Exception as e:
            st.error(f"âŒ æ•°æ®æ•´ç†å’Œæ‰“åŒ…å¤±è´¥: {str(e)}")
            st.exception(e)

def show_llm_processing_rules():
    """æ˜¾ç¤ºå¤§æ¨¡å‹å¤„ç†è§„åˆ™è®¾å®šåŠŸèƒ½"""
    st.subheader("ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™è®¾å®š")
    st.write("è®¾å®šå¤§æ¨¡å‹å¤„ç†æ•°æ®çš„è§„åˆ™å’Œæç¤ºè¯æ¨¡æ¿")
    
    # åˆå§‹åŒ–å¤„ç†è§„åˆ™
    if 'llm_processing_rules' not in st.session_state:
        st.session_state.llm_processing_rules = {
            "ç»¼åˆåˆ†æ": {
                "analysis_type": "comprehensive",
                "description": "ç»¼åˆåˆ†æï¼ŒåŒ…å«ä¸»é¢˜ã€æƒ…æ„Ÿã€æ´å¯Ÿå’Œç»“æ„åŒ–åˆ†æ",
                "prompt_template": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ·±åº¦åˆ†æRedditç¤¾åŒºä¸­å…³äºæŒ‡å®šä¸»é¢˜çš„è®¨è®ºã€‚

è¯·æ ¹æ®ä¸‹é¢æä¾›çš„åŸå§‹Redditå¸–å­å’Œè¯„è®ºæ•°æ®ï¼Œå®Œæˆä»¥ä¸‹å››ä¸ªéƒ¨åˆ†çš„ç»“æ„åŒ–åˆ†æå’Œæ€»ç»“ã€‚

### åŸå§‹æ•°æ®ï¼š{text}

### **ä»»åŠ¡ä¸€ï¼šæƒ…æ„Ÿä¸ç«‹åœºåˆ†æ (Sentiment & Stance)**
1. **æ•´ä½“æƒ…ç»ªï¼š** æ€»ç»“è¿™æ®µæ•°æ®æµä¸­ç”¨æˆ·è®¨è®ºçš„æ•´ä½“æƒ…ç»ªå€¾å‘
2. **æ ¸å¿ƒæƒ…æ„Ÿè¯†åˆ«ï¼š** è¯†åˆ«è®¨è®ºä¸­æœ€çªå‡ºçš„ä¸‰ç§æƒ…æ„Ÿ
3. **äº‰è®®ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼š** æ˜ç¡®æŒ‡å‡ºäº‰è®®çš„æ ¸å¿ƒç„¦ç‚¹

### **ä»»åŠ¡äºŒï¼šä¸»é¢˜ä¸ç—›ç‚¹æå– (Topic & Pain Points)**
1. **ä¸»è¦è®¨è®ºä¸»é¢˜ï¼š** å½’çº³ä¸º2åˆ°3ä¸ªæœ€é›†ä¸­çš„è®¨è®ºä¸»é¢˜
2. **æå–æ ¸å¿ƒç—›ç‚¹ï¼š** æ€»ç»“ç”¨æˆ·æœ€å¸¸è§ã€æœ€è¿«åˆ‡çš„é—®é¢˜æˆ–æŒ‘æˆ˜

### **ä»»åŠ¡ä¸‰ï¼šå®ç”¨å»ºè®®å’ŒæŠ€å·§å½’çº³ (Actionable Advice)**
1. **Top 5 å®ç”¨å»ºè®®ï¼š** æå–äº”æ¡æœ€å…·æ“ä½œæ€§çš„å»ºè®®
2. **å·¥å…·/å“ç‰ŒæåŠï¼š** æå–è¢«æåŠæœ€é¢‘ç¹çš„å·¥å…·ã€äº§å“æˆ–å“ç‰Œ

### **ä»»åŠ¡å››ï¼šç»“æ„åŒ–æ‘˜è¦ä¸æ€»ç»“ (Structured Output)**
ä»¥JSONæ ¼å¼è¾“å‡ºæœ€å…³é”®çš„æ´å¯Ÿ"""
            },
            "æƒ…æ„Ÿåˆ†æ": {
                "analysis_type": "sentiment",
                "description": "åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘å’Œæƒ…ç»ªçŠ¶æ€",
                "prompt_template": "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š{text}"
            },
            "ä¸»é¢˜åˆ†æ": {
                "analysis_type": "topic",
                "description": "æå–æ–‡æœ¬çš„ä¸»è¦è¯é¢˜å’Œå…³é”®è¯",
                "prompt_template": "è¯·æå–ä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦è¯é¢˜ï¼š{text}"
            },
            "è´¨é‡è¯„ä¼°": {
                "analysis_type": "quality",
                "description": "è¯„ä¼°å†…å®¹çš„è´¨é‡å’Œä»·å€¼",
                "prompt_template": "è¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹çš„è´¨é‡ï¼š{text}"
            }
        }
    
    # æ˜¾ç¤ºç°æœ‰è§„åˆ™
    st.write("**ç°æœ‰å¤„ç†è§„åˆ™:**")
    
    for rule_name, rule_config in st.session_state.llm_processing_rules.items():
        with st.expander(f"ğŸ“‹ {rule_name} - {rule_config['description']}"):
            col_rule1, col_rule2 = st.columns([3, 1])
            
            with col_rule1:
                st.write(f"**åˆ†æç±»å‹:** {rule_config['analysis_type']}")
                st.write(f"**æè¿°:** {rule_config['description']}")
                st.write("**æç¤ºè¯æ¨¡æ¿:**")
                st.code(rule_config['prompt_template'][:200] + "..." if len(rule_config['prompt_template']) > 200 else rule_config['prompt_template'])
            
            with col_rule2:
                if st.button("âœï¸ ç¼–è¾‘", key=create_unique_key("edit_rule", rule_name)):
                    toggle_state(f"editing_rule_{rule_name}", True)
                
                if st.button("ğŸ—‘ï¸ åˆ é™¤", key=create_unique_key("delete_rule", rule_name), type="secondary"):
                    if rule_name in st.session_state.llm_processing_rules:
                        del st.session_state.llm_processing_rules[rule_name]
                        st.rerun()
    
    # ç¼–è¾‘è§„åˆ™
    for rule_name in st.session_state.llm_processing_rules.keys():
        if st.session_state.get(f"editing_rule_{rule_name}", False):
            show_rule_editor(rule_name)
    
    # åˆ›å»ºæ–°è§„åˆ™
    st.write("---")
    st.write("**åˆ›å»ºæ–°å¤„ç†è§„åˆ™**")
    
    if st.button("â• åˆ›å»ºæ–°è§„åˆ™"):
        toggle_state("creating_new_rule", True)
    
    if st.session_state.get("creating_new_rule", False):
        show_rule_editor("new")

def show_rule_editor(rule_name):
    """æ˜¾ç¤ºè§„åˆ™ç¼–è¾‘å™¨"""
    if rule_name == "new":
        st.write("**åˆ›å»ºæ–°è§„åˆ™**")
        new_rule_name = st.text_input("è§„åˆ™åç§°", key="new_rule_name")
        new_analysis_type = st.selectbox("åˆ†æç±»å‹", ["comprehensive", "sentiment", "topic", "quality"], key="new_analysis_type")
        new_description = st.text_input("è§„åˆ™æè¿°", key="new_description")
        new_prompt = st.text_area("æç¤ºè¯æ¨¡æ¿", height=300, key="new_prompt", 
                                 value="è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š{text}")
        
        col_save, col_cancel = st.columns(2)
        
        with col_save:
            if st.button("ğŸ’¾ ä¿å­˜è§„åˆ™", key="save_new_rule"):
                if new_rule_name and new_prompt:
                    st.session_state.llm_processing_rules[new_rule_name] = {
                        "analysis_type": new_analysis_type,
                        "description": new_description,
                        "prompt_template": new_prompt
                    }
                    st.session_state["creating_new_rule"] = False
                    st.success("âœ… æ–°è§„åˆ™å·²åˆ›å»º")
                    st.rerun()
                else:
                    st.error("âŒ è¯·å¡«å†™è§„åˆ™åç§°å’Œæç¤ºè¯æ¨¡æ¿")
        
        with col_cancel:
            if st.button("âŒ å–æ¶ˆ", key="cancel_new_rule"):
                toggle_state("creating_new_rule", False)
    else:
        st.write(f"**ç¼–è¾‘è§„åˆ™: {rule_name}**")
        
        rule_config = st.session_state.llm_processing_rules[rule_name]
        
        edited_name = st.text_input("è§„åˆ™åç§°", value=rule_name, key=create_unique_key("edit_name", rule_name))
        edited_analysis_type = st.selectbox("åˆ†æç±»å‹", ["comprehensive", "sentiment", "topic", "quality"], 
                                          index=["comprehensive", "sentiment", "topic", "quality"].index(rule_config['analysis_type']),
                                          key=create_unique_key("edit_type", rule_name))
        edited_description = st.text_input("è§„åˆ™æè¿°", value=rule_config['description'], key=create_unique_key("edit_desc", rule_name))
        edited_prompt = st.text_area("æç¤ºè¯æ¨¡æ¿", value=rule_config['prompt_template'], height=300, key=create_unique_key("edit_prompt", rule_name))
        
        col_save, col_cancel = st.columns(2)
        
        with col_save:
            if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", key=create_unique_key("save_edit", rule_name)):
                if edited_name and edited_prompt:
                    # å¦‚æœåç§°æ”¹å˜äº†ï¼Œå…ˆåˆ é™¤æ—§è§„åˆ™
                    if edited_name != rule_name:
                        del st.session_state.llm_processing_rules[rule_name]
                    
                    # æ·»åŠ æ–°è§„åˆ™
                    st.session_state.llm_processing_rules[edited_name] = {
                        "analysis_type": edited_analysis_type,
                        "description": edited_description,
                        "prompt_template": edited_prompt
                    }
                    
                    st.session_state[f"editing_rule_{rule_name}"] = False
                    st.success("âœ… è§„åˆ™å·²æ›´æ–°")
                    st.rerun()
                else:
                    st.error("âŒ è¯·å¡«å†™è§„åˆ™åç§°å’Œæç¤ºè¯æ¨¡æ¿")
        
        with col_cancel:
            if st.button("âŒ å–æ¶ˆ", key=create_unique_key("cancel_edit", rule_name)):
                toggle_state(f"editing_rule_{rule_name}", False)

def show_manual_processing():
    """æ˜¾ç¤ºæ‰‹åŠ¨æ•°æ®å¤„ç†åŠŸèƒ½"""
    st.subheader("ğŸ”„ æ‰‹åŠ¨æ•°æ®å¤„ç†")
    st.write("æ‰‹åŠ¨é€‰æ‹©æ•°æ®å¹¶ä¼ é€’ç»™å¤§æ¨¡å‹è¿›è¡Œå¤„ç†")
    
    # æ•°æ®é€‰æ‹©
    st.write("**é€‰æ‹©è¦å¤„ç†çš„æ•°æ®**")
    
    col_select1, col_select2 = st.columns(2)
    
    with col_select1:
        # æŒ‰åˆ†ç»„é€‰æ‹©
        grouped_data = st.session_state.db.get_posts_grouped_by_date_subreddit()
        if grouped_data:
            selected_groups = st.multiselect(
                "é€‰æ‹©æ•°æ®åˆ†ç»„",
                options=list(grouped_data.keys()),
                help="é€‰æ‹©è¦å¤„ç†çš„æ•°æ®åˆ†ç»„"
            )
        else:
            st.info("æš‚æ— æ•°æ®åˆ†ç»„")
            selected_groups = []
    
    with col_select2:
        # æŒ‰å­ç‰ˆå—é€‰æ‹©
        available_subreddits = st.session_state.db.get_subreddit_list()
        selected_subreddits = st.multiselect(
            "é€‰æ‹©å­ç‰ˆå—",
            options=available_subreddits,
            help="é€‰æ‹©è¦å¤„ç†çš„å­ç‰ˆå—",
            key="manual_selected_subreddits"
        )
    
    # å¤„ç†è§„åˆ™é€‰æ‹©
    st.write("**ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™é€‰æ‹©**")
    
    processing_rules = st.session_state.get('llm_processing_rules', {})
    if not processing_rules:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹© 'ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™' æ¥è®¾ç½®å¤„ç†è§„åˆ™")
        st.info("ğŸ’¡ è®¾ç½®å¥½è§„åˆ™åï¼Œæ‚¨å°±å¯ä»¥åœ¨è¿™é‡Œé€‰æ‹©è§„åˆ™æ¥å¤„ç†æ•°æ®äº†")
        
        # æ·»åŠ å¿«é€Ÿè·³è½¬æŒ‰é’®
        if st.button("ğŸš€ ç«‹å³è®¾ç½®å¤„ç†è§„åˆ™", type="primary"):
            st.session_state.page_option = "ğŸ¤– å¤§æ¨¡å‹å¤„ç†è§„åˆ™"
            st.rerun()
        return
    
    # æ˜¾ç¤ºå¯ç”¨çš„è§„åˆ™
    st.info(f"ğŸ“‹ å½“å‰æœ‰ {len(processing_rules)} ä¸ªå¯ç”¨çš„å¤„ç†è§„åˆ™")
    
    col_rule1, col_rule2 = st.columns(2)
    
    with col_rule1:
        selected_rule = st.selectbox(
            "é€‰æ‹©å¤„ç†è§„åˆ™",
            list(processing_rules.keys()),
            key="manual_selected_rule",
            help="é€‰æ‹©è¦ä½¿ç”¨çš„å¤„ç†è§„åˆ™"
        )
        
        ai_provider = st.selectbox(
            "AIæä¾›å•†",
            ["openai", "anthropic", "deepseek"],
            key="manual_ai_provider",
            help="é€‰æ‹©AIæä¾›å•†"
        )
    
    with col_rule2:
        batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", min_value=1, max_value=200, value=25, key="manual_batch_size", help="æ¯æ‰¹å¤„ç†çš„å¸–å­æ•°é‡")
        use_custom_prompt = st.checkbox("ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯", key="manual_use_custom_prompt", help="æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯")
    
    if use_custom_prompt:
        custom_prompt = st.text_area(
            "è‡ªå®šä¹‰æç¤ºè¯",
            value=processing_rules[selected_rule]['prompt_template'],
            height=200,
            key="manual_custom_prompt",
            help="è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨{text}ä½œä¸ºæ•°æ®å ä½ç¬¦"
        )
    else:
        custom_prompt = processing_rules[selected_rule]['prompt_template']
    
    # æ˜¾ç¤ºé€‰ä¸­çš„è§„åˆ™ä¿¡æ¯
    st.info(f"ğŸ“‹ å·²é€‰æ‹©è§„åˆ™: **{selected_rule}** | åˆ†æç±»å‹: **{processing_rules[selected_rule]['analysis_type']}** | AIæä¾›å•†: **{ai_provider}**")
    
    # å¼€å§‹å¤„ç†
    if st.button("ğŸš€ å¼€å§‹æ‰‹åŠ¨å¤„ç†", type="primary"):
        try:
            # æ”¶é›†è¦å¤„ç†çš„æ•°æ®
            posts_to_process = []
            
            # ä»é€‰ä¸­çš„åˆ†ç»„æ”¶é›†æ•°æ®
            for group_key in selected_groups:
                if group_key in grouped_data:
                    group_info = grouped_data[group_key]
                    for post in group_info['posts']:
                        # åªå¤„ç†æœªåˆ†æçš„å¸–å­
                        if not post.analyzed:
                            posts_to_process.append({
                                'id': post.id,
                                'title': post.title,
                                'content': post.selftext or "",
                                'author': post.author,
                                'score': post.score,
                                'subreddit': post.subreddit,
                                'group': group_key
                            })
            
            # ä»é€‰ä¸­çš„å­ç‰ˆå—æ”¶é›†æ•°æ®
            if selected_subreddits:
                session = st.session_state.db.get_session()
                for subreddit in selected_subreddits:
                    # åªè·å–æœªåˆ†æçš„å¸–å­
                    posts = session.query(st.session_state.db.RedditPost).filter(
                        st.session_state.db.RedditPost.subreddit == subreddit,
                        st.session_state.db.RedditPost.analyzed == False
                    ).limit(batch_size).all()
                    
                    for post in posts:
                        # é¿å…é‡å¤
                        if not any(p['id'] == post.id for p in posts_to_process):
                            posts_to_process.append({
                                'id': post.id,
                                'title': post.title,
                                'content': post.selftext or "",
                                'author': post.author,
                                'score': post.score,
                                'subreddit': post.subreddit,
                                'group': f"subreddit_{subreddit}"
                            })
                session.close()
            
            if not posts_to_process:
                st.warning("æ²¡æœ‰é€‰æ‹©è¦å¤„ç†çš„æ•°æ®")
                return
            
            # é™åˆ¶å¤„ç†æ•°é‡
            posts_to_process = posts_to_process[:batch_size]
            
            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # æ‰§è¡Œå¤„ç†
            results = []
            for i, post_data in enumerate(posts_to_process):
                status_text.text(f"å¤„ç†å¸–å­: {post_data['title'][:50]}...")
                progress_bar.progress((i + 1) / len(posts_to_process))
                
                # å‡†å¤‡æ–‡æœ¬å†…å®¹
                text_content = f"æ ‡é¢˜: {post_data['title']}\nå†…å®¹: {post_data['content']}"
                
                # æ‰§è¡Œåˆ†æ
                if processing_rules[selected_rule]['analysis_type'] == 'comprehensive':
                    result = st.session_state.analyzer.analyze_comprehensive(
                        text_content, ai_provider, custom_prompt
                    )
                else:
                    result = st.session_state.analyzer.analyze_posts_batch(
                        [post_data], ai_provider, processing_rules[selected_rule]['analysis_type']
                    )
                
                if "error" not in result:
                    # ä¿å­˜ç»“æœ
                    st.session_state.db.save_analysis_result(
                        post_data['id'], "post", processing_rules[selected_rule]['analysis_type'],
                        str(result), ai_provider
                    )
                    results.append({
                        'post': post_data,
                        'result': result
                    })
                
                time.sleep(1)  # é¿å…APIé™åˆ¶
            
            progress_bar.progress(100)
            status_text.text("âœ… å¤„ç†å®Œæˆ!")
            
            st.success(f"ğŸ‰ æ‰‹åŠ¨å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªå¸–å­")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            st.write("**ğŸ“Š å¤„ç†ç»“æœ**")
            for i, item in enumerate(results[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç»“æœ
                with st.expander(f"ç»“æœ {i+1}: {item['post']['title'][:50]}..."):
                    st.json(item['result'])
            
            if len(results) > 5:
                st.info(f"è¿˜æœ‰ {len(results) - 5} ä¸ªç»“æœæœªæ˜¾ç¤º...")
            
        except Exception as e:
            st.error(f"âŒ æ‰‹åŠ¨å¤„ç†å¤±è´¥: {str(e)}")
            st.exception(e)

def show_results_display():
    """æ˜¾ç¤ºç»“æœå±•ç¤ºåŠŸèƒ½"""
    st.subheader("ğŸ“Š ç»“æœå±•ç¤º")
    
    # åˆ†æç»“æœç»Ÿè®¡
    st.subheader("ğŸ” åˆ†æç»“æœç»Ÿè®¡")
    
    try:
        results = st.session_state.db.get_analysis_results()
        
        if results:
            # æŒ‰åˆ†æç±»å‹åˆ†ç»„ç»Ÿè®¡
            analysis_stats = {}
            for result in results:
                if result.analysis_type not in analysis_stats:
                    analysis_stats[result.analysis_type] = 0
                analysis_stats[result.analysis_type] += 1
            
            # æ˜¾ç¤ºç»Ÿè®¡å›¾è¡¨
            if analysis_stats:
                st.bar_chart(analysis_stats)
            
            # æœ€è¿‘åˆ†æç»“æœ
            st.write("**æœ€è¿‘åˆ†æç»“æœ:**")
            recent_results = results[-20:]  # æœ€è¿‘20æ¡
            
            for result in recent_results:
                with st.expander(f"{result.analysis_type} - {result.content_id[:20]}... ({result.created_at.strftime('%m-%d %H:%M')})"):
                    try:
                        result_data = json.loads(result.result)
                        st.json(result_data)
                    except:
                        st.text(result.result)
        else:
            st.info("æš‚æ— åˆ†æç»“æœ")
            
    except Exception as e:
        st.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    # æ•°æ®å¯¼å‡ºåŒºåŸŸ
    st.write("---")
    st.subheader("ğŸ“¥ æ•°æ®å¯¼å‡º")
    
    # è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    try:
        stats = st.session_state.db.get_analysis_statistics()
        total_posts = stats.get('total_posts', 0)
        total_results = stats.get('total_analysis', 0)
        
        # è·å–æ•°æ®åŒ…æ•°é‡
        import os
        import glob
        output_dir = "output"
        package_count = 0
        if os.path.exists(output_dir):
            package_files = glob.glob(os.path.join(output_dir, "reddit_data_*.json"))
            package_count = len(package_files)
    except:
        total_posts = 0
        total_results = 0
        package_count = 0
    
    # æ•°æ®ç±»å‹é€‰æ‹©ï¼ˆå¤šé€‰åˆ—è¡¨ï¼‰
    st.write("**é€‰æ‹©è¦å¯¼å‡ºçš„æ•°æ®ç±»å‹:**")
    
    data_types = []
    if total_posts > 0:
        data_types.append(f"åŸå§‹æ•°æ® ({total_posts}æ¡)")
    if total_results > 0:
        data_types.append(f"åˆ†æç»“æœ ({total_results}æ¡)")
    if package_count > 0:
        data_types.append(f"æ•°æ®åŒ… ({package_count}ä¸ª)")
    
    if not data_types:
        st.info("æš‚æ— æ•°æ®å¯ä¾›å¯¼å‡º")
        return
    
    selected_data_types = st.multiselect(
        "æ•°æ®ç±»å‹",
        options=data_types,
        help="é€‰æ‹©è¦å¯¼å‡ºçš„æ•°æ®ç±»å‹ï¼Œå¯å¤šé€‰"
    )
    
    if not selected_data_types:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ç§æ•°æ®ç±»å‹")
        return
    
    # ç­›é€‰æ¡ä»¶
    col_filter1, col_filter2, col_format = st.columns(3)
    
    with col_filter1:
        # æ—¥æœŸèŒƒå›´é€‰æ‹©
        date_range = st.date_input(
            "é€‰æ‹©æ—¥æœŸèŒƒå›´",
            value=(datetime.now() - timedelta(days=30), datetime.now()),
            help="é€‰æ‹©è¦å¯¼å‡ºçš„æ•°æ®æ—¥æœŸèŒƒå›´"
        )
    
    with col_filter2:
        # å­ç‰ˆå—é€‰æ‹©
        available_subreddits = st.session_state.db.get_subreddit_list()
        selected_subreddits = st.multiselect(
            "é€‰æ‹©å­ç‰ˆå—",
            options=available_subreddits,
            help="é€‰æ‹©è¦å¯¼å‡ºçš„å­ç‰ˆå—ï¼Œç•™ç©ºè¡¨ç¤ºå…¨éƒ¨"
        )
    
    with col_format:
        # æ ¼å¼é€‰æ‹©
        export_format = st.selectbox(
            "å¯¼å‡ºæ ¼å¼",
            ["JSON", "CSV", "Excel"],
            help="é€‰æ‹©å¯¼å‡ºæ–‡ä»¶çš„æ ¼å¼"
        )
    
    # å¯¼å‡ºæ¨¡å¼é€‰æ‹©
    st.write("**å¯¼å‡ºæ¨¡å¼:**")
    col_mode1, col_mode2 = st.columns(2)
    
    with col_mode1:
        # ç®€åŒ–ä¸ºåªæœ‰å…¨é‡ä¸‹è½½æ¨¡å¼
        export_mode = "å…¨é‡ä¸‹è½½"
        st.info("ğŸ“¥ ä½¿ç”¨å…¨é‡ä¸‹è½½æ¨¡å¼ï¼Œç›´æ¥å¯¼å‡ºæ‰€æœ‰æ•°æ®")
    
    # å…¨é‡ä¸‹è½½æ¨¡å¼ï¼šæ˜¾ç¤ºå¯¼å‡ºæŒ‰é’®
    col_btn1, col_btn2 = st.columns(2)
    
    with col_btn1:
        if st.button("ğŸ“¥ æ‰¹é‡å¯¼å‡ºæ•°æ®", type="primary"):
            try:
                export_data_batch(selected_data_types, date_range, selected_subreddits, export_format, export_mode, None)
            except Exception as e:
                st.error(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
    
    with col_btn2:
        # Excelå¯¼å‡ºæŒ‰é’® - åªåœ¨å¤§æ¨¡å‹åˆ†ææ•°æ®åæ˜¾ç¤º
        if st.button("ğŸ“Š ç”ŸæˆExcelåˆ†ææŠ¥å‘Š", type="secondary", help="ç”ŸæˆåŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„ExcelæŠ¥å‘Š"):
            try:
                export_excel_report(date_range, selected_subreddits)
            except Exception as e:
                st.error(f"ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

def export_raw_data(date_range, selected_subreddits, export_format):
    """å¯¼å‡ºåŸå§‹æ•°æ®"""
    st.write("**æ­£åœ¨å¯¼å‡ºåŸå§‹æ•°æ®...**")
    
    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.RedditPost)
        
        # æ—¥æœŸç­›é€‰
        if date_range[0]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at <= date_range[1])
        
        # å­ç‰ˆå—ç­›é€‰
        if selected_subreddits:
            query = query.filter(st.session_state.db.RedditPost.subreddit.in_(selected_subreddits))
        
        posts = query.all()
        
        if not posts:
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # å‡†å¤‡æ•°æ®
        data = []
        for post in posts:
            data.append({
                'id': post.id,
                'title': post.title,
                'author': post.author,
                'score': post.score,
                'num_comments': post.num_comments,
                'created_utc': post.created_utc.isoformat() if post.created_utc else None,
                'subreddit': post.subreddit,
                'selftext': post.selftext,
                'url': post.url,
                'scraped_at': post.scraped_at.isoformat() if post.scraped_at else None
            })
        
        # ç”Ÿæˆæ–‡ä»¶å†…å®¹
        if export_format == "JSON":
            import json
            file_content = json.dumps(data, ensure_ascii=False, indent=2)
            file_extension = "json"
            mime_type = "application/json"
        else:  # CSV
            import pandas as pd
            df = pd.DataFrame(data)
            file_content = df.to_csv(index=False, encoding='utf-8-sig')
            file_extension = "csv"
            mime_type = "text/csv"
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reddit_raw_data_{timestamp}.{file_extension}"
        
        # æä¾›ä¸‹è½½
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½åŸå§‹æ•°æ® ({len(data)} æ¡è®°å½•)",
            data=file_content,
            file_name=filename,
            mime=mime_type
        )
        
        st.success(f"âœ… å·²å‡†å¤‡ {len(data)} æ¡åŸå§‹æ•°æ®ä¾›ä¸‹è½½")
        
    finally:
        session.close()

def export_analysis_results(date_range, selected_subreddits, export_format):
    """å¯¼å‡ºåˆ†æç»“æœ - å€Ÿé‰´Excelå¯¼å‡ºé€»è¾‘"""
    st.write("**æ­£åœ¨å¯¼å‡ºåˆ†æç»“æœ...**")
    
    try:
        # å€Ÿé‰´Excelå¯¼å‡ºé€»è¾‘ï¼šç›´æ¥ä½¿ç”¨get_analysis_results()è·å–æ‰€æœ‰æ•°æ®
        results = st.session_state.db.get_analysis_results()
        
        if not results:
            st.warning("æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœæ•°æ®")
            return
        
        # å‡†å¤‡æ•°æ®
        data = []
        session = st.session_state.db.get_session()
        try:
            for result in results:
                # è·å–å…³è”çš„å¸–å­ä¿¡æ¯
                post = session.query(st.session_state.db.RedditPost).filter(
                    st.session_state.db.RedditPost.id == result.content_id
                ).first()
                
                # æ—¥æœŸç­›é€‰
                if date_range[0] and result.created_at and result.created_at.date() < date_range[0]:
                    continue
                if date_range[1] and result.created_at and result.created_at.date() > date_range[1]:
                    continue
                
                # å­ç‰ˆå—ç­›é€‰
                if selected_subreddits and post and post.subreddit not in selected_subreddits:
                    continue
                
                data.append({
                    'analysis_id': result.id,
                    'content_id': result.content_id,
                    'content_type': result.content_type,
                    'analysis_type': result.analysis_type,
                    'model_used': result.model_used,
                    'created_at': result.created_at.isoformat() if result.created_at else None,
                    'result': result.result,
                    'post_title': post.title if post else None,
                    'post_subreddit': post.subreddit if post else None,
                    'post_author': post.author if post else None
                })
        finally:
            session.close()
        
        if not data:
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # ç”Ÿæˆæ–‡ä»¶å†…å®¹
        if export_format == "JSON":
            import json
            file_content = json.dumps(data, ensure_ascii=False, indent=2)
            file_extension = "json"
            mime_type = "application/json"
        else:  # CSV
            import pandas as pd
            df = pd.DataFrame(data)
            file_content = df.to_csv(index=False, encoding='utf-8-sig')
            file_extension = "csv"
            mime_type = "text/csv"
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analysis_results_{timestamp}.{file_extension}"
        
        # æä¾›ä¸‹è½½
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½åˆ†æç»“æœ ({len(data)} æ¡è®°å½•)",
            data=file_content,
            file_name=filename,
            mime=mime_type
        )
        
        st.success(f"âœ… å·²å‡†å¤‡ {len(data)} æ¡åˆ†æç»“æœä¾›ä¸‹è½½")
        
    finally:
        session.close()

def export_data_packages(date_range, selected_subreddits, export_format):
    """å¯¼å‡ºæ•°æ®åŒ…"""
    st.write("**æ­£åœ¨å¯¼å‡ºæ•°æ®åŒ…...**")
    
    # è·å–outputç›®å½•ä¸‹çš„æ•°æ®åŒ…æ–‡ä»¶
    import os
    import glob
    
    output_dir = "output"
    if not os.path.exists(output_dir):
        st.warning("æ²¡æœ‰æ‰¾åˆ°æ•°æ®åŒ…æ–‡ä»¶")
        return
    
    # æŸ¥æ‰¾æ•°æ®åŒ…æ–‡ä»¶
    pattern = os.path.join(output_dir, "reddit_data_*.json")
    package_files = glob.glob(pattern)
    
    if not package_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°æ•°æ®åŒ…æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºå¯ç”¨çš„æ•°æ®åŒ…æ–‡ä»¶
    st.write("**å¯ç”¨çš„æ•°æ®åŒ…æ–‡ä»¶:**")
    for i, file_path in enumerate(package_files):
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) / 1024  # KB
        st.write(f"{i+1}. {file_name} ({file_size:.1f} KB)")
    
    # æä¾›æ‰¹é‡ä¸‹è½½
    if st.button("ğŸ“¦ æ‰“åŒ…ä¸‹è½½æ‰€æœ‰æ•°æ®åŒ…"):
        import zipfile
        import io
        
        # åˆ›å»ºZIPæ–‡ä»¶
        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for file_path in package_files:
                file_name = os.path.basename(file_path)
                zip_file.write(file_path, file_name)
        
        zip_buffer.seek(0)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"reddit_data_packages_{timestamp}.zip"
        
        # æä¾›ä¸‹è½½
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½æ‰€æœ‰æ•°æ®åŒ… ({len(package_files)} ä¸ªæ–‡ä»¶)",
            data=zip_buffer.getvalue(),
            file_name=zip_filename,
            mime="application/zip"
        )
        
        st.success(f"âœ… å·²å‡†å¤‡ {len(package_files)} ä¸ªæ•°æ®åŒ…ä¾›ä¸‹è½½")

def export_data_batch(selected_data_types, date_range, selected_subreddits, export_format, export_mode, preview_limit=None):
    """æ‰¹é‡å¯¼å‡ºæ•°æ®"""
    st.write("**æ­£åœ¨å‡†å¤‡æ‰¹é‡å¯¼å‡º...**")
    
    # è§£æé€‰ä¸­çš„æ•°æ®ç±»å‹
    data_to_export = []
    for data_type in selected_data_types:
        if "åŸå§‹æ•°æ®" in data_type:
            data_to_export.append("raw_data")
        elif "åˆ†æç»“æœ" in data_type:
            data_to_export.append("analysis_results")
        elif "æ•°æ®åŒ…" in data_type:
            data_to_export.append("data_packages")
    
    if export_mode == "å…¨é‡ä¸‹è½½":
        # å…¨é‡ä¸‹è½½æ¨¡å¼
        export_all_data(data_to_export, date_range, selected_subreddits, export_format)
    else:
        # é¢„è§ˆé€‰æ‹©æ¨¡å¼ - å¯¼å‡ºæ—¶ä½¿ç”¨å…¨é‡æ•°æ®
        preview_and_select_data(data_to_export, date_range, selected_subreddits, export_format, None)

def export_all_data(data_to_export, date_range, selected_subreddits, export_format):
    """å…¨é‡å¯¼å‡ºæ‰€æœ‰é€‰ä¸­çš„æ•°æ®"""
    import zipfile
    import io
    
    # å¦‚æœæ˜¯Excelæ ¼å¼ï¼Œç›´æ¥ç”ŸæˆExcelæŠ¥å‘Š
    if export_format == "Excel":
        export_excel_report(date_range, selected_subreddits)
        return
    
    zip_buffer = io.BytesIO()
    file_count = 0
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for data_type in data_to_export:
            if data_type == "raw_data":
                file_content, filename = get_raw_data_content(date_range, selected_subreddits, export_format)
                if file_content:
                    zip_file.writestr(filename, file_content)
                    file_count += 1
            
            elif data_type == "analysis_results":
                file_content, filename = get_analysis_results_content(date_range, selected_subreddits, export_format)
                if file_content:
                    zip_file.writestr(filename, file_content)
                    file_count += 1
            
            elif data_type == "data_packages":
                # æ•°æ®åŒ…ç›´æ¥å¤åˆ¶æ–‡ä»¶
                import os
                import glob
                output_dir = "output"
                if os.path.exists(output_dir):
                    package_files = glob.glob(os.path.join(output_dir, "reddit_data_*.json"))
                    for file_path in package_files:
                        file_name = os.path.basename(file_path)
                        with open(file_path, 'r', encoding='utf-8') as f:
                            zip_file.writestr(file_name, f.read())
                        file_count += 1
    
    if file_count > 0:
        zip_buffer.seek(0)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"reddit_data_export_{timestamp}.zip"
        
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½æ‰¹é‡å¯¼å‡ºæ–‡ä»¶ ({file_count} ä¸ªæ–‡ä»¶)",
            data=zip_buffer.getvalue(),
            file_name=zip_filename,
            mime="application/zip"
        )
        
        st.success(f"âœ… å·²å‡†å¤‡ {file_count} ä¸ªæ–‡ä»¶ä¾›ä¸‹è½½")
    else:
        st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")

def preview_and_select_data(data_to_export, date_range, selected_subreddits, export_format, preview_limit):
    """é¢„è§ˆå¹¶é€‰æ‹©æ•°æ®"""
    st.write("**æ•°æ®é¢„è§ˆå’Œé€‰æ‹©**")
    
    for data_type in data_to_export:
        if data_type == "raw_data":
            preview_raw_data_with_pagination(date_range, selected_subreddits, export_format, preview_limit)
        elif data_type == "analysis_results":
            preview_analysis_results_with_pagination(date_range, selected_subreddits, export_format, preview_limit)
        elif data_type == "data_packages":
            preview_data_packages()

def preview_raw_data_with_pagination(date_range, selected_subreddits, export_format, preview_limit):
    """å¸¦åˆ†é¡µå’Œç­›é€‰çš„åŸå§‹æ•°æ®é¢„è§ˆ"""
    st.write("**åŸå§‹æ•°æ®é¢„è§ˆ**")
    
    # åˆå§‹åŒ–åˆ†é¡µçŠ¶æ€
    if 'raw_data_page' not in st.session_state:
        st.session_state.raw_data_page = 0
    if 'raw_data_selected' not in st.session_state:
        st.session_state.raw_data_selected = []
    
    # åˆ›å»ºç­›é€‰æ¡ä»¶çš„å”¯ä¸€æ ‡è¯†
    filter_key = f"{date_range}_{selected_subreddits}_{preview_limit}"
    if 'raw_data_filter_key' not in st.session_state:
        st.session_state.raw_data_filter_key = filter_key
    elif st.session_state.raw_data_filter_key != filter_key:
        # ç­›é€‰æ¡ä»¶æ”¹å˜ï¼Œé‡ç½®åˆ†é¡µçŠ¶æ€
        st.session_state.raw_data_page = 0
        st.session_state.raw_data_filter_key = filter_key
    
    # é«˜çº§ç­›é€‰æ¡ä»¶
    st.write("**é«˜çº§ç­›é€‰æ¡ä»¶**")
    col_filter1, col_filter2, col_filter3 = st.columns(3)
    
    with col_filter1:
        keyword_filter = st.text_input("å…³é”®è¯ç­›é€‰", help="åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢å…³é”®è¯", key="raw_keyword_filter")
        min_score = st.number_input("æœ€å°åˆ†æ•°", min_value=0, value=0, help="ç­›é€‰åˆ†æ•°å¤§äºç­‰äºæ­¤å€¼çš„å¸–å­", key="raw_min_score")
    
    with col_filter2:
        author_filter = st.text_input("ä½œè€…ç­›é€‰", help="ç­›é€‰ç‰¹å®šä½œè€…å‘å¸ƒçš„å¸–å­", key="raw_author_filter")
        max_score = st.number_input("æœ€å¤§åˆ†æ•°", min_value=0, value=10000, help="ç­›é€‰åˆ†æ•°å°äºç­‰äºæ­¤å€¼çš„å¸–å­", key="raw_max_score")
    
    with col_filter3:
        min_comments = st.number_input("æœ€å°è¯„è®ºæ•°", min_value=0, value=0, help="ç­›é€‰è¯„è®ºæ•°å¤§äºç­‰äºæ­¤å€¼çš„å¸–å­", key="raw_min_comments")
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["æœ€æ–°", "æœ€æ—§", "åˆ†æ•°æœ€é«˜", "åˆ†æ•°æœ€ä½", "è¯„è®ºæœ€å¤š"], help="é€‰æ‹©æ•°æ®æ’åºæ–¹å¼", key="raw_sort_by")
    
    # åº”ç”¨ç­›é€‰æŒ‰é’®
    col_apply1, col_apply2, col_apply3 = st.columns([1, 1, 2])
    with col_apply1:
        if st.button("ğŸ” åº”ç”¨ç­›é€‰", key="apply_raw_filter"):
            # åº”ç”¨ç­›é€‰æ¡ä»¶ï¼Œé‡ç½®åˆ†é¡µçŠ¶æ€
            st.session_state.raw_data_page = 0
            st.session_state.raw_data_advanced_filter_key = f"{keyword_filter}_{author_filter}_{min_score}_{max_score}_{min_comments}_{sort_by}"
            st.rerun()
    
    with col_apply2:
        if st.button("ğŸ”„ é‡ç½®ç­›é€‰", key="reset_raw_filter"):
            # é‡ç½®ç­›é€‰æ¡ä»¶
            st.session_state.raw_data_page = 0
            st.session_state.raw_data_advanced_filter_key = ""
            st.rerun()
    
    # è·å–å½“å‰åº”ç”¨çš„ç­›é€‰æ¡ä»¶
    current_advanced_filter = st.session_state.get('raw_data_advanced_filter_key', '')
    if current_advanced_filter:
        st.info(f"å½“å‰ç­›é€‰æ¡ä»¶: {current_advanced_filter}")
    
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.RedditPost)
        
        # åŸºç¡€ç­›é€‰æ¡ä»¶
        if date_range[0]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at <= date_range[1])
        if selected_subreddits:
            query = query.filter(st.session_state.db.RedditPost.subreddit.in_(selected_subreddits))
        
        # åº”ç”¨é«˜çº§ç­›é€‰æ¡ä»¶ï¼ˆä½¿ç”¨å·²ä¿å­˜çš„ç­›é€‰æ¡ä»¶ï¼‰
        if current_advanced_filter:
            # è§£æå·²ä¿å­˜çš„ç­›é€‰æ¡ä»¶
            filter_parts = current_advanced_filter.split('_')
            if len(filter_parts) >= 6:
                saved_keyword = filter_parts[0] if filter_parts[0] else ""
                saved_author = filter_parts[1] if filter_parts[1] else ""
                saved_min_score = int(filter_parts[2]) if filter_parts[2] else 0
                saved_max_score = int(filter_parts[3]) if filter_parts[3] else 10000
                saved_min_comments = int(filter_parts[4]) if filter_parts[4] else 0
                saved_sort_by = filter_parts[5] if filter_parts[5] else "æœ€æ–°"
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if saved_keyword:
                    query = query.filter(
                        (st.session_state.db.RedditPost.title.contains(saved_keyword)) |
                        (st.session_state.db.RedditPost.selftext.contains(saved_keyword))
                    )
                if saved_author:
                    query = query.filter(st.session_state.db.RedditPost.author.contains(saved_author))
                if saved_min_score > 0:
                    query = query.filter(st.session_state.db.RedditPost.score >= saved_min_score)
                if saved_max_score < 10000:
                    query = query.filter(st.session_state.db.RedditPost.score <= saved_max_score)
                if saved_min_comments > 0:
                    query = query.filter(st.session_state.db.RedditPost.num_comments >= saved_min_comments)
                
                # åº”ç”¨æ’åº
                if saved_sort_by == "æœ€æ–°":
                    query = query.order_by(st.session_state.db.RedditPost.scraped_at.desc())
                elif saved_sort_by == "æœ€æ—§":
                    query = query.order_by(st.session_state.db.RedditPost.scraped_at.asc())
                elif saved_sort_by == "åˆ†æ•°æœ€é«˜":
                    query = query.order_by(st.session_state.db.RedditPost.score.desc())
                elif saved_sort_by == "åˆ†æ•°æœ€ä½":
                    query = query.order_by(st.session_state.db.RedditPost.score.asc())
                elif saved_sort_by == "è¯„è®ºæœ€å¤š":
                    query = query.order_by(st.session_state.db.RedditPost.num_comments.desc())
        
        total_count = query.count()
        
        if total_count == 0:
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # åˆ†é¡µè®¡ç®—
        total_pages = (total_count + preview_limit - 1) // preview_limit
        current_page = st.session_state.raw_data_page
        
        # åˆ†é¡µæ§åˆ¶
        col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
        
        with col_page1:
            if st.button("â¬…ï¸ ä¸Šä¸€é¡µ", disabled=current_page == 0):
                st.session_state.raw_data_page = max(0, current_page - 1)
                st.rerun()
        
        with col_page2:
            st.write(f"**ç¬¬ {current_page + 1} é¡µ / å…± {total_pages} é¡µ** | æ€»è®¡ {total_count} æ¡æ•°æ®")
        
        with col_page3:
            if st.button("ä¸‹ä¸€é¡µ â¡ï¸", disabled=current_page >= total_pages - 1):
                st.session_state.raw_data_page = min(total_pages - 1, current_page + 1)
                st.rerun()
        
        # è·å–å½“å‰é¡µæ•°æ®
        offset = current_page * preview_limit
        posts = query.offset(offset).limit(preview_limit).all()
        
        # å…¨é€‰åŠŸèƒ½
        col_select1, col_select2 = st.columns([1, 1])
        with col_select1:
            if st.button(f"âœ… å…¨é€‰å½“å‰é¡µ ({len(posts)} æ¡)"):
                for post in posts:
                    if post.id not in st.session_state.raw_data_selected:
                        st.session_state.raw_data_selected.append(post.id)
                st.rerun()
        
        with col_select2:
            if st.button(f"âœ… å…¨é€‰æ‰€æœ‰æ•°æ® ({total_count} æ¡)"):
                all_posts = query.all()
                st.session_state.raw_data_selected = [post.id for post in all_posts]
                st.rerun()
        
        # æ˜¾ç¤ºå½“å‰é¡µæ•°æ®
        if posts:
            for i, post in enumerate(posts):
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(f"**{post.title[:80]}...** | r/{post.subreddit} | åˆ†æ•°: {post.score} | è¯„è®º: {post.num_comments}")
                    if post.author:
                        st.write(f"ä½œè€…: u/{post.author} | æ—¶é—´: {post.scraped_at.strftime('%Y-%m-%d %H:%M') if post.scraped_at else 'æœªçŸ¥'}")
                with col2:
                    is_selected = post.id in st.session_state.raw_data_selected
                    if st.checkbox("é€‰æ‹©", value=is_selected, key=f"raw_post_{post.id}"):
                        if post.id not in st.session_state.raw_data_selected:
                            st.session_state.raw_data_selected.append(post.id)
                    else:
                        if post.id in st.session_state.raw_data_selected:
                            st.session_state.raw_data_selected.remove(post.id)
        
        # æ˜¾ç¤ºå·²é€‰æ‹©çš„æ•°æ®ç»Ÿè®¡
        if st.session_state.raw_data_selected:
            st.info(f"å·²é€‰æ‹© {len(st.session_state.raw_data_selected)} æ¡æ•°æ®")
            
            if st.button(f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„ {len(st.session_state.raw_data_selected)} æ¡åŸå§‹æ•°æ®", key="download_selected_raw"):
                # è·å–é€‰ä¸­çš„å¸–å­æ•°æ®
                selected_posts = session.query(st.session_state.db.RedditPost).filter(
                    st.session_state.db.RedditPost.id.in_(st.session_state.raw_data_selected)
                ).all()
                export_selected_posts(selected_posts, export_format)
        
    finally:
        session.close()

def preview_raw_data(date_range, selected_subreddits, export_format, preview_limit):
    """é¢„è§ˆåŸå§‹æ•°æ®"""
    st.write("**åŸå§‹æ•°æ®é¢„è§ˆ**")
    
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.RedditPost)
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if date_range[0]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at <= date_range[1])
        if selected_subreddits:
            query = query.filter(st.session_state.db.RedditPost.subreddit.in_(selected_subreddits))
        
        total_count = query.count()
        # å¦‚æœæ˜¯å¯¼å‡ºæ¨¡å¼ï¼Œä¸é™åˆ¶æ•°é‡ï¼›å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œé™åˆ¶æ•°é‡
        if preview_limit is None:  # å¯¼å‡ºæ¨¡å¼ï¼Œè·å–æ‰€æœ‰æ•°æ®
            posts = query.all()
        else:  # é¢„è§ˆæ¨¡å¼ï¼Œé™åˆ¶æ•°é‡
            posts = query.limit(preview_limit).all()
        
        st.info(f"æ‰¾åˆ° {total_count} æ¡åŸå§‹æ•°æ®ï¼Œæ˜¾ç¤ºå‰ {len(posts)} æ¡")
        
        if posts:
            # åˆ›å»ºé€‰æ‹©æ¡†
            selected_posts = []
            for i, post in enumerate(posts):
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(f"**{post.title[:80]}...** | r/{post.subreddit} | åˆ†æ•°: {post.score}")
                with col2:
                    if st.checkbox("é€‰æ‹©", key=f"raw_post_{post.id}"):
                        selected_posts.append(post)
            
            if selected_posts:
                if st.button(f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„ {len(selected_posts)} æ¡åŸå§‹æ•°æ®", key="download_selected_raw"):
                    export_selected_posts(selected_posts, export_format)
            
            if total_count > preview_limit:
                st.info(f"è¿˜æœ‰ {total_count - preview_limit} æ¡æ•°æ®æœªæ˜¾ç¤ºï¼Œå¦‚éœ€æŸ¥çœ‹æ›´å¤šè¯·è°ƒæ•´é¢„è§ˆæ•°é‡é™åˆ¶")
        
    finally:
        session.close()

def preview_analysis_results_with_pagination(date_range, selected_subreddits, export_format, preview_limit):
    """å¸¦åˆ†é¡µå’Œç­›é€‰çš„åˆ†æç»“æœé¢„è§ˆ"""
    st.write("**åˆ†æç»“æœé¢„è§ˆ**")
    
    # åˆå§‹åŒ–åˆ†é¡µçŠ¶æ€
    if 'results_page' not in st.session_state:
        st.session_state.results_page = 0
    if 'results_selected' not in st.session_state:
        st.session_state.results_selected = []
    
    # åˆ›å»ºç­›é€‰æ¡ä»¶çš„å”¯ä¸€æ ‡è¯†
    filter_key = f"{date_range}_{selected_subreddits}_{preview_limit}"
    if 'results_filter_key' not in st.session_state:
        st.session_state.results_filter_key = filter_key
    elif st.session_state.results_filter_key != filter_key:
        # ç­›é€‰æ¡ä»¶æ”¹å˜ï¼Œé‡ç½®åˆ†é¡µçŠ¶æ€
        st.session_state.results_page = 0
        st.session_state.results_filter_key = filter_key
    
    # é«˜çº§ç­›é€‰æ¡ä»¶
    st.write("**é«˜çº§ç­›é€‰æ¡ä»¶**")
    col_filter1, col_filter2, col_filter3 = st.columns(3)
    
    with col_filter1:
        analysis_type_filter = st.multiselect(
            "åˆ†æç±»å‹ç­›é€‰",
            ["comprehensive", "sentiment", "topic", "quality"],
            help="é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ†æç±»å‹",
            key="results_analysis_type_filter"
        )
        model_filter = st.selectbox(
            "æ¨¡å‹ç­›é€‰",
            ["å…¨éƒ¨", "openai", "anthropic", "deepseek"],
            help="é€‰æ‹©ä½¿ç”¨çš„AIæ¨¡å‹",
            key="results_model_filter"
        )
    
    with col_filter2:
        keyword_filter = st.text_input("å…³é”®è¯ç­›é€‰", help="åœ¨å¸–å­æ ‡é¢˜ä¸­æœç´¢å…³é”®è¯", key="results_keyword_filter")
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["æœ€æ–°", "æœ€æ—§", "åˆ†æç±»å‹"], help="é€‰æ‹©æ•°æ®æ’åºæ–¹å¼", key="results_sort_by")
    
    with col_filter3:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šç­›é€‰æ¡ä»¶
        pass
    
    # åº”ç”¨ç­›é€‰æŒ‰é’®
    col_apply1, col_apply2, col_apply3 = st.columns([1, 1, 2])
    with col_apply1:
        if st.button("ğŸ” åº”ç”¨ç­›é€‰", key="apply_results_filter"):
            # åº”ç”¨ç­›é€‰æ¡ä»¶ï¼Œé‡ç½®åˆ†é¡µçŠ¶æ€
            st.session_state.results_page = 0
            st.session_state.results_advanced_filter_key = f"{analysis_type_filter}_{model_filter}_{keyword_filter}_{sort_by}"
            st.rerun()
    
    with col_apply2:
        if st.button("ğŸ”„ é‡ç½®ç­›é€‰", key="reset_results_filter"):
            # é‡ç½®ç­›é€‰æ¡ä»¶
            st.session_state.results_page = 0
            st.session_state.results_advanced_filter_key = ""
            st.rerun()
    
    # è·å–å½“å‰åº”ç”¨çš„ç­›é€‰æ¡ä»¶
    current_advanced_filter = st.session_state.get('results_advanced_filter_key', '')
    if current_advanced_filter:
        st.info(f"å½“å‰ç­›é€‰æ¡ä»¶: {current_advanced_filter}")
    
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.AnalysisResult)
        
        # åŸºç¡€ç­›é€‰æ¡ä»¶
        if date_range[0]:
            query = query.filter(st.session_state.db.AnalysisResult.created_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.AnalysisResult.created_at <= date_range[1])
        
        # åº”ç”¨é«˜çº§ç­›é€‰æ¡ä»¶ï¼ˆä½¿ç”¨å·²ä¿å­˜çš„ç­›é€‰æ¡ä»¶ï¼‰
        if current_advanced_filter:
            # è§£æå·²ä¿å­˜çš„ç­›é€‰æ¡ä»¶
            filter_parts = current_advanced_filter.split('_')
            if len(filter_parts) >= 4:
                saved_analysis_types = filter_parts[0] if filter_parts[0] else ""
                saved_model = filter_parts[1] if filter_parts[1] else ""
                saved_keyword = filter_parts[2] if filter_parts[2] else ""
                saved_sort_by = filter_parts[3] if filter_parts[3] else "æœ€æ–°"
                
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if saved_analysis_types:
                    analysis_types = saved_analysis_types.split(',') if ',' in saved_analysis_types else [saved_analysis_types]
                    query = query.filter(st.session_state.db.AnalysisResult.analysis_type.in_(analysis_types))
                if saved_model != "å…¨éƒ¨":
                    query = query.filter(st.session_state.db.AnalysisResult.model_used == saved_model)
                
                # åº”ç”¨æ’åº
                if saved_sort_by == "æœ€æ–°":
                    query = query.order_by(st.session_state.db.AnalysisResult.created_at.desc())
                elif saved_sort_by == "æœ€æ—§":
                    query = query.order_by(st.session_state.db.AnalysisResult.created_at.asc())
                elif saved_sort_by == "åˆ†æç±»å‹":
                    query = query.order_by(st.session_state.db.AnalysisResult.analysis_type)
        
        results = query.all()
        total_count = len(results)
        
        # åº”ç”¨å­ç‰ˆå—å’Œå…³é”®è¯ç­›é€‰
        filtered_results = []
        for result in results:
            post = session.query(st.session_state.db.RedditPost).filter(
                st.session_state.db.RedditPost.id == result.content_id
            ).first()
            
            # å­ç‰ˆå—ç­›é€‰
            if selected_subreddits and post and post.subreddit not in selected_subreddits:
                continue
            
            # å…³é”®è¯ç­›é€‰ï¼ˆä½¿ç”¨å·²ä¿å­˜çš„ç­›é€‰æ¡ä»¶ï¼‰
            if current_advanced_filter:
                filter_parts = current_advanced_filter.split('_')
                if len(filter_parts) >= 3:
                    saved_keyword = filter_parts[2] if filter_parts[2] else ""
                    if saved_keyword and post and saved_keyword.lower() not in post.title.lower():
                        continue
            
            filtered_results.append(result)
        
        total_count = len(filtered_results)
        
        if total_count == 0:
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # åˆ†é¡µè®¡ç®—
        if preview_limit is None:  # å¯¼å‡ºæ¨¡å¼ï¼Œä¸åˆ†é¡µ
            total_pages = 1
            current_page = 0
        else:  # é¢„è§ˆæ¨¡å¼ï¼Œè®¡ç®—åˆ†é¡µ
            total_pages = (total_count + preview_limit - 1) // preview_limit
            current_page = st.session_state.results_page
        
        # åˆ†é¡µæ§åˆ¶
        col_page1, col_page2, col_page3 = st.columns([1, 2, 1])
        
        with col_page1:
            if st.button("â¬…ï¸ ä¸Šä¸€é¡µ", disabled=current_page == 0, key="results_prev"):
                st.session_state.results_page = max(0, current_page - 1)
                st.rerun()
        
        with col_page2:
            st.write(f"**ç¬¬ {current_page + 1} é¡µ / å…± {total_pages} é¡µ** | æ€»è®¡ {total_count} æ¡æ•°æ®")
        
        with col_page3:
            if st.button("ä¸‹ä¸€é¡µ â¡ï¸", disabled=current_page >= total_pages - 1, key="results_next"):
                st.session_state.results_page = min(total_pages - 1, current_page + 1)
                st.rerun()
        
        # è·å–å½“å‰é¡µæ•°æ®
        if preview_limit is None:  # å¯¼å‡ºæ¨¡å¼ï¼Œè·å–æ‰€æœ‰æ•°æ®
            page_results = filtered_results
        else:  # é¢„è§ˆæ¨¡å¼ï¼Œä½¿ç”¨åˆ†é¡µ
            offset = current_page * preview_limit
            page_results = filtered_results[offset:offset + preview_limit]
        
        # å…¨é€‰åŠŸèƒ½
        col_select1, col_select2 = st.columns([1, 1])
        with col_select1:
            if st.button(f"âœ… å…¨é€‰å½“å‰é¡µ ({len(page_results)} æ¡)", key="select_page_results"):
                for result in page_results:
                    if result.id not in st.session_state.results_selected:
                        st.session_state.results_selected.append(result.id)
                st.rerun()
        
        with col_select2:
            if st.button(f"âœ… å…¨é€‰æ‰€æœ‰æ•°æ® ({total_count} æ¡)", key="select_all_results"):
                st.session_state.results_selected = [result.id for result in filtered_results]
                st.rerun()
        
        # æ˜¾ç¤ºå½“å‰é¡µæ•°æ®
        if page_results:
            for i, result in enumerate(page_results):
                post = session.query(st.session_state.db.RedditPost).filter(
                    st.session_state.db.RedditPost.id == result.content_id
                ).first()
                
                col1, col2 = st.columns([4, 1])
                with col1:
                    post_title = post.title[:60] + "..." if post and post.title else "æœªçŸ¥å¸–å­"
                    st.write(f"**{result.analysis_type}** | {post_title} | {result.model_used}")
                    if post:
                        st.write(f"r/{post.subreddit} | åˆ†æ•°: {post.score} | æ—¶é—´: {result.created_at.strftime('%Y-%m-%d %H:%M') if result.created_at else 'æœªçŸ¥'}")
                with col2:
                    is_selected = result.id in st.session_state.results_selected
                    if st.checkbox("é€‰æ‹©", value=is_selected, key=f"result_{result.id}"):
                        if result.id not in st.session_state.results_selected:
                            st.session_state.results_selected.append(result.id)
                    else:
                        if result.id in st.session_state.results_selected:
                            st.session_state.results_selected.remove(result.id)
        
        # æ˜¾ç¤ºå·²é€‰æ‹©çš„æ•°æ®ç»Ÿè®¡
        if st.session_state.results_selected:
            st.info(f"å·²é€‰æ‹© {len(st.session_state.results_selected)} æ¡åˆ†æç»“æœ")
            
            if st.button(f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„ {len(st.session_state.results_selected)} æ¡åˆ†æç»“æœ", key="download_selected_results"):
                # è·å–é€‰ä¸­çš„åˆ†æç»“æœæ•°æ®
                selected_results = session.query(st.session_state.db.AnalysisResult).filter(
                    st.session_state.db.AnalysisResult.id.in_(st.session_state.results_selected)
                ).all()
                export_selected_results(selected_results, export_format)
        
    finally:
        session.close()

def preview_analysis_results(date_range, selected_subreddits, export_format, preview_limit):
    """é¢„è§ˆåˆ†æç»“æœ"""
    st.write("**åˆ†æç»“æœé¢„è§ˆ**")
    
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.AnalysisResult)
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if date_range[0]:
            query = query.filter(st.session_state.db.AnalysisResult.created_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.AnalysisResult.created_at <= date_range[1])
        
        # å¦‚æœæ˜¯å¯¼å‡ºæ¨¡å¼ï¼Œä¸é™åˆ¶æ•°é‡ï¼›å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œé™åˆ¶æ•°é‡
        if preview_limit is None:  # å¯¼å‡ºæ¨¡å¼ï¼Œè·å–æ‰€æœ‰æ•°æ®
            results = query.all()
        else:  # é¢„è§ˆæ¨¡å¼ï¼Œé™åˆ¶æ•°é‡
            results = query.limit(preview_limit).all()
        total_count = query.count()
        
        st.info(f"æ‰¾åˆ° {total_count} æ¡åˆ†æç»“æœï¼Œæ˜¾ç¤ºå‰ {len(results)} æ¡")
        
        if results:
            selected_results = []
            for i, result in enumerate(results):
                # è·å–å…³è”çš„å¸–å­ä¿¡æ¯
                post = session.query(st.session_state.db.RedditPost).filter(
                    st.session_state.db.RedditPost.id == result.content_id
                ).first()
                
                # å­ç‰ˆå—ç­›é€‰
                if selected_subreddits and post and post.subreddit not in selected_subreddits:
                    continue
                
                col1, col2 = st.columns([4, 1])
                with col1:
                    post_title = post.title[:60] + "..." if post and post.title else "æœªçŸ¥å¸–å­"
                    st.write(f"**{result.analysis_type}** | {post_title} | {result.model_used}")
                with col2:
                    if st.checkbox("é€‰æ‹©", key=f"result_{result.id}"):
                        selected_results.append(result)
            
            if selected_results:
                if st.button(f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„ {len(selected_results)} æ¡åˆ†æç»“æœ", key="download_selected_results"):
                    export_selected_results(selected_results, export_format)
            
            if total_count > preview_limit:
                st.info(f"è¿˜æœ‰ {total_count - preview_limit} æ¡ç»“æœæœªæ˜¾ç¤ºï¼Œå¦‚éœ€æŸ¥çœ‹æ›´å¤šè¯·è°ƒæ•´é¢„è§ˆæ•°é‡é™åˆ¶")
        
    finally:
        session.close()

def preview_data_packages():
    """é¢„è§ˆæ•°æ®åŒ…"""
    st.write("**æ•°æ®åŒ…é¢„è§ˆ**")
    
    import os
    import glob
    
    output_dir = "output"
    if not os.path.exists(output_dir):
        st.warning("æ²¡æœ‰æ‰¾åˆ°æ•°æ®åŒ…æ–‡ä»¶")
        return
    
    package_files = glob.glob(os.path.join(output_dir, "reddit_data_*.json"))
    
    if package_files:
        st.info(f"æ‰¾åˆ° {len(package_files)} ä¸ªæ•°æ®åŒ…æ–‡ä»¶")
        
        selected_packages = []
        for i, file_path in enumerate(package_files):
            file_name = os.path.basename(file_path)
            file_size = os.path.getsize(file_path) / 1024  # KB
            
            col1, col2 = st.columns([4, 1])
            with col1:
                st.write(f"**{file_name}** ({file_size:.1f} KB)")
            with col2:
                if st.checkbox("é€‰æ‹©", key=f"package_{i}"):
                    selected_packages.append(file_path)
        
        if selected_packages:
            if st.button(f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„ {len(selected_packages)} ä¸ªæ•°æ®åŒ…", key="download_selected_packages"):
                export_selected_packages(selected_packages)
    else:
        st.warning("æ²¡æœ‰æ‰¾åˆ°æ•°æ®åŒ…æ–‡ä»¶")

def get_raw_data_content(date_range, selected_subreddits, export_format):
    """è·å–åŸå§‹æ•°æ®å†…å®¹"""
    session = st.session_state.db.get_session()
    try:
        query = session.query(st.session_state.db.RedditPost)
        
        if date_range[0]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at >= date_range[0])
        if date_range[1]:
            query = query.filter(st.session_state.db.RedditPost.scraped_at <= date_range[1])
        if selected_subreddits:
            query = query.filter(st.session_state.db.RedditPost.subreddit.in_(selected_subreddits))
        
        posts = query.all()
        
        if not posts:
            return None, None
        
        data = []
        for post in posts:
            data.append({
                'id': post.id,
                'title': post.title,
                'author': post.author,
                'score': post.score,
                'num_comments': post.num_comments,
                'created_utc': post.created_utc.isoformat() if post.created_utc else None,
                'subreddit': post.subreddit,
                'selftext': post.selftext,
                'url': post.url,
                'scraped_at': post.scraped_at.isoformat() if post.scraped_at else None
            })
        
        if export_format == "JSON":
            import json
            file_content = json.dumps(data, ensure_ascii=False, indent=2)
            file_extension = "json"
        else:  # CSV
            import pandas as pd
            df = pd.DataFrame(data)
            file_content = df.to_csv(index=False, encoding='utf-8-sig')
            file_extension = "csv"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reddit_raw_data_{timestamp}.{file_extension}"
        
        return file_content, filename
        
    finally:
        session.close()

def get_analysis_results_content(date_range, selected_subreddits, export_format):
    """è·å–åˆ†æç»“æœå†…å®¹ - å€Ÿé‰´Excelå¯¼å‡ºé€»è¾‘"""
    try:
        # å€Ÿé‰´Excelå¯¼å‡ºé€»è¾‘ï¼šç›´æ¥ä½¿ç”¨get_analysis_results()è·å–æ‰€æœ‰æ•°æ®
        results = st.session_state.db.get_analysis_results()
        
        if not results:
            return None, None
        
        data = []
        session = st.session_state.db.get_session()
        try:
            for result in results:
                post = session.query(st.session_state.db.RedditPost).filter(
                    st.session_state.db.RedditPost.id == result.content_id
                ).first()
                
                # æ—¥æœŸç­›é€‰
                if date_range[0] and result.created_at and result.created_at.date() < date_range[0]:
                    continue
                if date_range[1] and result.created_at and result.created_at.date() > date_range[1]:
                    continue
                
                # å­ç‰ˆå—ç­›é€‰
                if selected_subreddits and post and post.subreddit not in selected_subreddits:
                    continue
                
                data.append({
                    'analysis_id': result.id,
                    'content_id': result.content_id,
                    'content_type': result.content_type,
                    'analysis_type': result.analysis_type,
                    'model_used': result.model_used,
                    'created_at': result.created_at.isoformat() if result.created_at else None,
                    'result': result.result,
                    'post_title': post.title if post else None,
                    'post_subreddit': post.subreddit if post else None,
                    'post_author': post.author if post else None
                })
        finally:
            session.close()
        
        if not data:
            return None, None
        
        if export_format == "JSON":
            import json
            file_content = json.dumps(data, ensure_ascii=False, indent=2)
            file_extension = "json"
        else:  # CSV
            import pandas as pd
            df = pd.DataFrame(data)
            file_content = df.to_csv(index=False, encoding='utf-8-sig')
            file_extension = "csv"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analysis_results_{timestamp}.{file_extension}"
        
        return file_content, filename
        
    finally:
        session.close()

def export_selected_posts(selected_posts, export_format):
    """å¯¼å‡ºé€‰ä¸­çš„å¸–å­"""
    data = []
    for post in selected_posts:
        data.append({
            'id': post.id,
            'title': post.title,
            'author': post.author,
            'score': post.score,
            'num_comments': post.num_comments,
            'created_utc': post.created_utc.isoformat() if post.created_utc else None,
            'subreddit': post.subreddit,
            'selftext': post.selftext,
            'url': post.url,
            'scraped_at': post.scraped_at.isoformat() if post.scraped_at else None
        })
    
    if export_format == "JSON":
        import json
        file_content = json.dumps(data, ensure_ascii=False, indent=2)
        file_extension = "json"
        mime_type = "application/json"
    else:  # CSV
        import pandas as pd
        df = pd.DataFrame(data)
        file_content = df.to_csv(index=False, encoding='utf-8-sig')
        file_extension = "csv"
        mime_type = "text/csv"
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"selected_posts_{timestamp}.{file_extension}"
    
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„å¸–å­ ({len(data)} æ¡)",
        data=file_content,
        file_name=filename,
        mime=mime_type
    )

def export_selected_results(selected_results, export_format):
    """å¯¼å‡ºé€‰ä¸­çš„åˆ†æç»“æœ"""
    session = st.session_state.db.get_session()
    try:
        data = []
        for result in selected_results:
            post = session.query(st.session_state.db.RedditPost).filter(
                st.session_state.db.RedditPost.id == result.content_id
            ).first()
            
            data.append({
                'analysis_id': result.id,
                'content_id': result.content_id,
                'content_type': result.content_type,
                'analysis_type': result.analysis_type,
                'model_used': result.model_used,
                'created_at': result.created_at.isoformat() if result.created_at else None,
                'result': result.result,
                'post_title': post.title if post else None,
                'post_subreddit': post.subreddit if post else None,
                'post_author': post.author if post else None
            })
        
        if export_format == "JSON":
            import json
            file_content = json.dumps(data, ensure_ascii=False, indent=2)
            file_extension = "json"
            mime_type = "application/json"
        else:  # CSV
            import pandas as pd
            df = pd.DataFrame(data)
            file_content = df.to_csv(index=False, encoding='utf-8-sig')
            file_extension = "csv"
            mime_type = "text/csv"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"selected_results_{timestamp}.{file_extension}"
        
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„åˆ†æç»“æœ ({len(data)} æ¡)",
            data=file_content,
            file_name=filename,
            mime=mime_type
        )
        
    finally:
        session.close()

def export_selected_packages(selected_packages):
    """å¯¼å‡ºé€‰ä¸­çš„æ•°æ®åŒ…"""
    import zipfile
    import io
    
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_path in selected_packages:
            file_name = os.path.basename(file_path)
            with open(file_path, 'r', encoding='utf-8') as f:
                zip_file.writestr(file_name, f.read())
    
    zip_buffer.seek(0)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename = f"selected_packages_{timestamp}.zip"
    
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è½½é€‰ä¸­çš„æ•°æ®åŒ… ({len(selected_packages)} ä¸ª)",
        data=zip_buffer.getvalue(),
        file_name=zip_filename,
        mime="application/zip"
    )

def export_excel_report(date_range, selected_subreddits):
    """å¯¼å‡ºExcelåˆ†ææŠ¥å‘Š"""
    st.write("**æ­£åœ¨ç”ŸæˆExcelåˆ†ææŠ¥å‘Š...**")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœæ•°æ®
        results = st.session_state.db.get_analysis_results()
        if not results:
            st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœæ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œå¤§æ¨¡å‹åˆ†æ")
            return
        
        # å‡†å¤‡æ—¥æœŸå‚æ•°
        start_date = None
        end_date = None
        if date_range and len(date_range) == 2:
            start_date = date_range[0].strftime('%Y-%m-%d') if date_range[0] else None
            end_date = date_range[1].strftime('%Y-%m-%d') if date_range[1] else None
        
        # å‡†å¤‡å­ç‰ˆå—å‚æ•°
        subreddits = selected_subreddits if selected_subreddits else None
        
        # åˆå§‹åŒ–æ•°æ®æ•´ç†å™¨
        from data_organizer import DataOrganizer
        organizer = DataOrganizer(st.session_state.db, st.session_state.analyzer)
        
        # ç”ŸæˆExcelæŠ¥å‘Š
        with st.spinner("æ­£åœ¨ç”ŸæˆExcelæŠ¥å‘Šï¼Œè¯·ç¨å€™..."):
            excel_file_path = organizer.generate_excel_report(
                start_date=start_date,
                end_date=end_date,
                subreddits=subreddits,
                output_dir="output"
            )
        
        # è¯»å–ç”Ÿæˆçš„Excelæ–‡ä»¶
        with open(excel_file_path, 'rb') as f:
            excel_data = f.read()
        
        # ç”Ÿæˆä¸‹è½½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reddit_analysis_report_{timestamp}.xlsx"
        
        # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
        st.success("âœ… Excelåˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆä¿¡æ¯
        st.info(f"""
        ğŸ“Š **æŠ¥å‘ŠåŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨ï¼š**
        - **ç»¼åˆåˆ†æç»“æœ**ï¼šåŒ…å«æ‰€æœ‰å¸–å­çš„è¯¦ç»†åˆ†æç»“æœ
        - **ç»Ÿè®¡æ¦‚è§ˆ**ï¼šæ•°æ®ç»Ÿè®¡å’Œåˆ†ææ¬¡æ•°ç»Ÿè®¡
        - **å­ç‰ˆå—åˆ†æ**ï¼šæŒ‰å­ç‰ˆå—åˆ†ç»„çš„ç»Ÿè®¡æ•°æ®
        - **æƒ…æ„Ÿåˆ†æ**ï¼šæƒ…æ„Ÿåˆ†æç»“æœç»Ÿè®¡
        - **ä¸»é¢˜åˆ†æ**ï¼šä¸»é¢˜å…³é”®è¯ç»Ÿè®¡
        
        ğŸ“… **æ•°æ®èŒƒå›´ï¼š** {start_date or 'ä¸é™'} è‡³ {end_date or 'ä¸é™'}
        ğŸ·ï¸ **å­ç‰ˆå—ï¼š** {', '.join(subreddits) if subreddits else 'å…¨éƒ¨'}
        """)
        
        # æä¾›ä¸‹è½½æŒ‰é’®
        st.download_button(
            label="ğŸ“Š ä¸‹è½½Excelåˆ†ææŠ¥å‘Š",
            data=excel_data,
            file_name=filename,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            help="ç‚¹å‡»ä¸‹è½½åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„ExcelæŠ¥å‘Š"
        )
        
    except Exception as e:
        st.error(f"âŒ ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        st.exception(e)
